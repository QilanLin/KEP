\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{url}
% Prevent URL line breaks
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
\Urlmuskip=0mu plus 1mu
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes}
\usepackage{algorithm2e}
\usepackage{enumitem}

% Page setup - narrow margins
\geometry{left=0.75in,right=0.75in,top=0.75in,bottom=0.75in}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

% Disable hyphenation completely - words that don't fit go to next line
\hyphenpenalty=10000
\exhyphenpenalty=10000
\doublehyphendemerits=10000
\finalhyphendemerits=10000
\adjdemerits=10000
% Allow more flexible line spacing to avoid hyphenation
\tolerance=9999
\emergencystretch=10em
\hbadness=10000
\hfuzz=10pt
\sloppy

% Code listings setup - Isabelle language with symbol mapping
\lstdefinelanguage{isabelle}{
    basicstyle=\ttfamily\small,
    columns=fullflexible,
    keepspaces=true,
    mathescape=true,
    numbers=none,  % Disable line numbers to avoid confusion with breaklines
    literate=
        {\\<forall>}{{$\forall$}}1
        {\\<exists>}{{$\exists$}}1
        {\\<Rightarrow>}{{$\Rightarrow$}}1
        {\\<Longrightarrow>}{{$\Longrightarrow$}}1
        {==>}{{$\Rightarrow$}}3
        {=>}{{$\Rightarrow$}}2
        {\\<or>}{{$\lor$}}1
        {\\<and>}{{$\land$}}1
        {\\<longrightarrow>}{{$\longrightarrow$}}1
        {\\<noteq>}{{$\neq$}}1,
    commentstyle=\color{gray}\itshape,
    showspaces=false,
    showstringspaces=false,
    tabsize=2
}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=Python,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    showspaces=false,
    showstringspaces=false,
    tabsize=2
}

% Title information
\title{Testing the Isabelle Sledgehammer Ecosystem: \\
A Dual Approach to Prover and Integration Fuzzing}
\author{Qilan Lin (K21204786) \\
        Department of Informatics, \\
        King's College London \\
        \texttt{qilan.lin@kcl.ac.uk}}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
Proof assistants like Isabelle/HOL rely on complex ecosystems involving external automated theorem provers (ATPs) and SMT solvers integrated through the Sledgehammer interface. While the interface layer has received significant engineering attention, the reliability of the underlying provers themselves directly impacts Sledgehammer's usability. This paper presents a holistic testing framework that addresses both prover reliability and interface robustness through a dual testing approach.

Our framework makes two key contributions: (1) a \textit{differential testing methodology} that discovered 519 performance bugs across three major provers (E Prover, cvc5, Z3), representing timeouts, errors, and performance degradation; and (2) an \textit{AST-based mutation fuzzer} for Isabelle theories with 10 intelligent mutation operators specifically designed for testing Sledgehammer integration, validated using Mirabelle as the authoritative testing tool.

Experimental evaluation on 214 mutations validated by Mirabelle (Isabelle's official testing tool) found no integration bugs, suggesting the integration layer is robust within the tested mutation space. In contrast, differential testing exposed substantial volatility in underlying provers (519 performance bugs).

\textbf{Keywords:} Fuzzing, Isabelle/HOL, Sledgehammer, Differential Testing, AST Mutation, Test Oracles, Theorem Provers
\end{abstract}

\section{Introduction}

Proof assistants such as Isabelle/HOL play a critical role in formal verification, enabling the verification of safety-critical systems from operating system kernels (Klein et al., 2009) to cryptographic protocols. Isabelle's Sledgehammer tool (Paulson \& Blanchette, 2012) significantly enhances productivity by automatically invoking external automated theorem provers (ATPs) and SMT solvers to find proofs. However, the reliability of this entire ecosystem---both the integration layer and the underlying provers---is crucial for trustworthy verification.

\subsection{Motivation}

The Sledgehammer ecosystem presents two distinct reliability challenges:

\textbf{Prover Reliability}: External provers (E Prover, cvc5, Z3, Vampire) form the computational foundation of Sledgehammer. Performance issues in these provers---timeouts, crashes, or errors---directly impact Sledgehammer's ability to find proofs. If a prover times out or fails on a provable problem, Sledgehammer cannot assist the user, reducing proof automation effectiveness.

\textbf{Integration Reliability}: The interface layer between Isabelle and external provers involves TPTP/SMT-LIB encoding, prover invocation, output parsing, and proof reconstruction. Bugs at this layer can cause crashes, incorrect results, or reconstruction failures even when provers function correctly.

While traditional fuzzing has proven effective for general software testing, existing approaches have two key limitations for proof assistant testing:

\begin{enumerate}
    \item \textbf{Lack of prover-specific testing}: General fuzzing tools treat provers as black boxes, missing performance degradation patterns that matter for proof automation.
    \item \textbf{High false positive rates}: Naive bug detection may misclassify warnings as errors or theory-level issues as integration bugs.
\end{enumerate}

\subsection{Contributions}

This paper presents an extensive testing framework that addresses both challenges through a methodical dual approach combining prover reliability testing with integration robustness evaluation.

We developed a differential testing methodology specifically designed for automated theorem prover performance evaluation. By rigorously comparing the behavior of three major provers (E Prover, cvc5, and Z3) across over 1000 TPTP problems, we discovered 519 performance bugs that directly impact Sledgehammer's usability. These bugs are distributed across E Prover (349 bugs, representing 67.2\% of the total), cvc5 (143 bugs, 27.6\%), and Z3 (27 bugs, 5.2\%). The bugs are categorized by type into timeouts (288 bugs, 55.5\% of total), errors (115 bugs, 22.2\%), and slowdowns (116 bugs, 22.3\%). To our knowledge, this represents one of the largest reproducible, structured cross-prover differential-performance bug discovery campaigns in the Sledgehammer-relevant ATP/SMT setting, providing concrete evidence of widespread performance issues that affect real-world proof automation.

We also developed a complete AST-based fuzzer specifically designed for testing Sledgehammer integration through Isabelle theory mutation. Unlike existing fuzzing tools that operate on bytes or tokens without understanding structure, our fuzzer parses Isabelle theories to extract lemmas and their components, then applies grammar-aware transformations that maintain syntactic validity. The fuzzer implements ten distinct mutation operators targeting different aspects of the integration layer, including quantifier transformations, logical operator modifications, term reordering, and proof method variations. We generated and tested 214 mutations across 10 carefully selected seed theories, demonstrating that our fuzzer can methodically explore the input space while maintaining syntactic validity.

To ensure accurate bug detection, we use Mirabelle (Isabelle's official testing tool for Sledgehammer) as the authoritative validator. Mirabelle directly determines whether mutations reveal integration bugs by examining Sledgehammer's execution, distinguishing integration failures from theory-level errors.

Finally, our \textbf{systematic testing campaign} provides empirical evidence about Sledgehammer's integration layer \textbf{within the tested mutation space}. Testing over 214 diverse mutations validated by Mirabelle found \textbf{no integration bugs}, suggesting the interface handles these tested structural transformations robustly. This stands in stark contrast to the backend provers (519 performance bugs), suggesting reliability concerns may be more concentrated in the computational layer than in the interface implementation within our test scope.

\section{Background and Related Work}

\subsection{Isabelle Sledgehammer}

Isabelle Sledgehammer (Paulson \& Blanchette, 2012) automates proof discovery by:
\begin{enumerate}
    \item Selecting relevant facts from the proof context
    \item Encoding HOL formulas into TPTP or SMT-LIB
    \item Invoking external provers (E, Vampire, Z3, cvc5, etc.)
    \item Parsing prover responses
    \item Reconstructing proofs using Metis or SMT replay
\end{enumerate}

Our testing targets both the underlying provers (steps 3) and the integration layer (steps 2, 4, 5).

\subsection{Fuzzing and Differential Testing}

\textbf{Coverage-Guided Fuzzing}: AFL (AFL Technical Details, n.d.) and LibFuzzer (LLVM Project, n.d.) use coverage feedback to guide mutation, effective for finding crashes but not semantic bugs.

\textbf{Differential Testing}: Used successfully for compilers (Yang et al., 2011) and SMT solvers (Brummayer \& Biere, 2009), comparing outputs from different implementations to detect bugs. Our differential approach extends this to theorem prover performance testing.

\textbf{Structure-Aware Fuzzing}: ODDFUZZ (2023) for Java deserialization and SQUIRREL (2020) for databases demonstrate benefits of grammar-aware mutation. Our AST-based approach applies this to logical formulas.

\subsection{Gap Analysis}

\begin{table}[h]
\centering
\caption{Comparison with Existing Approaches}
\label{tab:comparison}
\scalebox{0.85}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Prover Testing} & \textbf{Integration Testing} & \textbf{False Positive Control} & \textbf{Official Validation} \\
\midrule
AFL/LibFuzzer & Limited & No & No & No \\
SMT-LIB Fuzzers & Yes & No & Limited & No \\
Manual Testing & Ad-hoc & Ad-hoc & N/A & Sometimes \\
\textbf{This Work} & \textbf{Systematic (519 bugs)} & \textbf{Yes (214 tests)} & \textbf{Yes (Mirabelle)} & \textbf{Yes (Official Tool)} \\
\bottomrule
\end{tabular}%
}
\end{table}

As Table~\ref{tab:comparison} illustrates, prior approaches address individual aspects but none provide a unified framework that methodically tests prover performance at scale, specifically targets integration layer reliability through fuzzing, rigorously eliminates false positives, and validates findings using official tools.

\section{Methodology}

\subsection{Part A: Prover Differential Testing}

\subsubsection{Test Suite}

Our prover testing methodology relies on an extensive test suite comprising over 1000 problems drawn from diverse domains. These problems span fundamental areas including arithmetic reasoning, set theory operations, quantified formulas, and real-world verification scenarios. The diversity of this test suite is crucial for exposing performance issues across different problem characteristics, as prover performance often varies considerably depending on the logical structure, quantifier complexity, and domain-specific features of the input problem.

The selection of TPTP as our canonical benchmark source is deliberate: TPTP (Thousands of Problems for Theorem Provers) is the de facto standard interchange format for first-order logic problems in the automated reasoning community. Each problem is translated into the prover's native input format (TPTP for E Prover; SMT-LIB for cvc5 and Z3) before execution. This preserves comparability while respecting tool interfaces and facilitates reproducibility with other testing efforts.

\subsubsection{Differential Oracle}

Our differential testing oracle operates by executing each problem across three major provers---E Prover, cvc5, and Z3---and methodically comparing their behaviors to detect performance anomalies. For each problem $p$, we run all three provers concurrently with identical timeout settings and computational resources (using native TPTP format for E Prover and SMT-LIB translations for the SMT solvers), then analyze the results to identify discrepancies that indicate potential bugs.

\begin{algorithm}[H]
\caption{Differential Testing Oracle}
\label{alg:differential}
\SetAlgoLined
\KwData{TPTP problem $p$, provers $\{E, cvc5, Z3\}$, timeout $t$}
\KwResult{Bug report or PASS}
\For{each prover $P$ in $\{E, cvc5, Z3\}$}{
    $result_P \gets \text{RunProver}(P, p, t)$\;
}
\If{any prover timed out while others succeeded}{
    \Return TIMEOUT\_BUG($p$, prover)\;
}
\If{any prover errored while others succeeded}{
    \Return ERROR\_BUG($p$, prover)\;
}
\If{execution times vary significantly}{
    \Return SLOWDOWN\_BUG($p$, prover)\;
}
\Return PASS\;
\end{algorithm}

The differential oracle classifies bugs into three distinct categories based on the nature of the performance discrepancy observed. \textbf{Timeout bugs} occur when a prover exceeds the configured timeout threshold of 30 seconds while at least one of the other provers successfully completes the same problem. This pattern indicates that the problem is provable (as demonstrated by the successful prover), but the timing-out prover either employs an inefficient strategy or encounters a performance regression. Such bugs are particularly problematic for Sledgehammer users, as they prevent automated proof discovery even when a proof exists.

\textbf{Error bugs} manifest when a prover returns a non-zero exit code or produces error messages while other provers succeed on the same problem. These errors may stem from parser failures, internal assertion violations, or unhandled edge cases in the prover's logic. Unlike timeouts, which merely indicate slowness, errors represent actual failures in the prover's execution that could potentially indicate deeper issues such as soundness problems or implementation bugs.

\textbf{Slowdown bugs} represent cases where a prover successfully solves the problem but takes significantly longer than alternative provers. We define ``significantly longer'' as execution time exceeding 10 times the median execution time of successful provers on the same problem. Slowdown bugs are reported only when at least two provers succeed; the reference median is computed over the \textit{other} successful provers excluding the target prover. While these cases do not represent complete failures, they indicate suboptimal performance that degrades user experience and reduces the efficiency of automated proof search in Sledgehammer.

\subsection{Part B: Integration Fuzzing}

\subsubsection{AST-Based Mutation}

Unlike TPTP-level mutation, we mutate Isabelle theories directly at the AST level:

\begin{algorithm}[H]
\caption{Isabelle Theory Mutation}
\label{alg:mutation}
\SetAlgoLined
\KwData{Seed theory $T$, mutation count $n$}
\KwResult{List of mutants $M$}
$M \gets \emptyset$\;
$lemmas \gets \text{ExtractLemmas}(T)$\;
\For{$i = 1$ to $n$}{
    $lemma \gets \text{RandomChoice}(lemmas)$\;
    $type \gets \text{RandomChoice}(\text{MutationTypes})$\;
    $mutant \gets \text{ApplyMutation}(T, lemma, type)$\;
    \If{$mutant \neq T$ and $mutant \notin M$}{
        $M \gets M \cup \{mutant\}$\;
    }
}
\Return $M$\;
\end{algorithm}

Our mutation engine implements ten distinct mutation operators designed to test different aspects of the Sledgehammer integration layer. These operators can be grouped into several categories based on the types of transformations they perform.

\begin{lstlisting}[language=isabelle, caption=Example Mutations Across Different Categories]
(* Logical structure mutation: Quantifier flipping *)
lemma original_1: "(\<forall>x. P x) ==> Q"
lemma mutated_1: "(\<exists>x. P x) ==> Q"  (* Forall to Exists *)

(* Term-level mutation: Argument reordering *)
lemma original_2: "f x y = g a b"
lemma mutated_2: "f y x = g b a"  (* Test commutativity *)

(* Proof-level mutation: Explicit Sledgehammer call *)
lemma original_3: "P ==> Q" by auto
lemma mutated_3: "P ==> Q" 
  by (sledgehammer)  (* Test direct invocation *)
\end{lstlisting}

Logical structure mutations target quantifiers and connectives, testing how Sledgehammer encodes different logical constructs into TPTP format. Operators in this category include quantifier flipping (exchanging universal and existential quantifiers), formula negation (applying logical negation to change lemma polarity), and conjunction swapping (exchanging AND and OR operators). These transformations typically change lemma semantics substantially, revealing potential issues in how Sledgehammer handles different quantifier types and logical connectives during encoding and proof search.

Term-level mutations focus on function arguments and constants, exploring how Sledgehammer's term encoding interacts with prover behavior. These include term reordering (swapping function arguments to test sensitivity to argument order), identity addition (inserting semantically neutral operations like adding zero), and constant replacement (substituting constants with different values, often invalidating lemmas to test error handling). These mutations test whether Sledgehammer and external provers can correctly handle semantically equivalent but syntactically different formulas, as well as how they respond to invalid inputs.

Proof-level mutations modify proof tactics and structure, testing Sledgehammer's interaction with Isabelle's proof engines. This category includes proof method changes (varying the proof tactic used), explicit Sledgehammer invocation (directly calling Sledgehammer in proof scripts), lemma duplication (creating redundant lemmas to test namespace handling), and assumption addition (introducing additional hypotheses). These operators explore how Sledgehammer behaves when called explicitly versus implicitly, and how it handles varying proof structures and lemma dependencies.

Each operator targets specific potential failure modes in the integration layer, from TPTP encoding errors to proof reconstruction issues, enabling thorough testing of Sledgehammer's resilience across diverse input patterns.

\subsection{Part C: Mirabelle Validation}

For integration bug detection, we use Mirabelle (Bulwahn \& Krauss, 2011), Isabelle's official testing tool specifically designed for evaluating automated proof tools like Sledgehammer. Mirabelle provides authoritative judgments on integration correctness by executing theories through Sledgehammer and detecting failures in the integration layer. As an official tool maintained by the Isabelle team, Mirabelle's verdicts serve as ground truth for integration bug detection, eliminating concerns about false positives or misclassification that could arise from custom detection logic.

\section{Implementation}

\subsection{Technology Stack}

Our framework is implemented in Python 3.13, comprising approximately 2000 lines of carefully engineered code organized into modular components. We chose Python for its excellent subprocess management capabilities (essential for invoking external provers and Isabelle), rich ecosystem of testing libraries, and clear syntax that facilitates rapid prototyping and iterative development.

The codebase is organized around core components that handle different aspects of the testing workflow. For differential testing, the crash and timeout detection oracle monitors prover execution for abnormal termination and timeout violations, while the differential oracle performs multi-prover comparison by parsing and normalizing outputs from different provers to detect result inconsistencies. The theory mutation engine parses Isabelle theory files and applies mutation operators while tracking validity, enabling structured exploration of the input space. For integration testing, we integrate directly with Mirabelle for bug detection and validation.

The fuzzing campaign orchestrator coordinates the complete automated workflow, managing mutation generation, test execution, and statistical reporting. Integration bug detection is handled directly by Mirabelle, Isabelle's official testing tool, which provides authoritative validation of Sledgehammer's behavior on each mutated theory.

Supporting this core implementation is an extensive testing infrastructure. We have developed over 20 unit tests covering success cases, error scenarios, boundary conditions, and edge cases, achieving good coverage of the critical code paths. Type annotations are present in over 95\% of function signatures, enabling static type checking and improving code maintainability. Additionally, we have written approximately 450 lines of detailed docstrings that document module purposes, function behaviors, usage examples, and design rationales, making the codebase self-documenting and accessible to future developers.

\subsection{Quality Assurance}

Ensuring the reliability of a testing framework is paramount, as bugs in the testing tool itself can lead to missed bugs or false alarms. We therefore invested significant effort in code quality across multiple dimensions to ensure production-quality software suitable for continuous integration and long-term maintenance.

We developed a custom exception hierarchy with specialized exception types for different error scenarios, enabling precise error handling and providing users with clear, actionable error messages rather than generic exceptions.

Input validation is performed rigorously before any external command execution. Theory names are validated against format requirements, length constraints, and reserved word lists, while file paths are validated for existence, readability, and type. All validation failures produce informative error messages that guide users toward correct usage.

Error handling throughout the codebase follows best practices, avoiding bare exception clauses and using specific exception types with exception chaining to preserve stack traces. Security considerations are addressed through command injection prevention, path traversal checks, and safe file operations using context managers to ensure proper cleanup even during exceptions.

Resource management is handled rigorously, with temporary files cleaned up automatically using try-finally blocks, subprocess cleanup ensuring no orphaned processes, and proper handling of file descriptors to prevent resource leaks during long-running fuzzing campaigns.

\section{Experimental Evaluation}

\subsection{Experimental Setup}

Our experimental evaluation consists of two complementary testing campaigns. For prover testing, we assembled a test suite of over 1000 TPTP problems representing diverse logical domains and difficulty levels. We tested three widely-used provers: E Prover version 3.1, cvc5 version 1.0.8, and Z3 version 4.12.2. Each problem was executed with a timeout of 30 seconds per test. All experiments were conducted on macOS 26.1 running on Apple M3 Max hardware with 64GB RAM.

For integration fuzzing, we curated a collection of 10 high-quality Isabelle theories to serve as mutation seeds. These seed theories were carefully selected to cover diverse Isabelle features including basic arithmetic operations, list manipulations, set operations, logical formulas, inductive proofs, function definitions, option types, pairs, number theory, and relations. Each seed theory contains multiple lemmas with proven proofs, ensuring that our baseline is valid Isabelle code. From these seeds, we generated mutations using our AST-based fuzzer, yielding a total of 214 mutated theories for testing. All mutations were validated using the official Mirabelle tool, providing authoritative verification of our testing results.

\subsection{Results}

\subsubsection{Prover Testing Results}

\begin{table}[h]
\centering
\caption{Prover Bugs Discovered (519 Total)}
\label{tab:prover_bugs}
\begin{tabular}{lrrrr}
\toprule
\textbf{Prover} & \textbf{Timeout} & \textbf{Error} & \textbf{Slowdown} & \textbf{Total} \\
\midrule
E Prover & 186 (53.3\%) & 67 (19.2\%) & 96 (27.5\%) & 349 (67.2\%) \\
cvc5 & 83 (58.0\%) & 41 (28.7\%) & 19 (13.3\%) & 143 (27.6\%) \\
Z3 & 19 (70.4\%) & 7 (25.9\%) & 1 (3.7\%) & 27 (5.2\%) \\
\midrule
\textbf{Total} & \textbf{288 (55.5\%)} & \textbf{115 (22.2\%)} & \textbf{116 (22.3\%)} & \textbf{519 (100\%)} \\
\bottomrule
\end{tabular}
\end{table}

Our differential testing campaign revealed several critical insights into prover reliability. Most strikingly, E Prover accounts for 67.2\% of all discovered bugs (349 out of 519) despite being one of the most widely deployed and mature theorem provers in the automated reasoning community. Given that all three provers were tested on identical benchmarks, this high count suggests that E Prover's strategy portfolio is more timeout-prone on this problem suite, possibly due to heavier search exploration or less effective strategy scheduling on certain problem types. The bug distribution also reveals that E Prover is particularly susceptible to timeout issues, with 186 timeout bugs representing 53.3\% of its total bugs.

Timeout bugs emerged as the most prevalent issue category overall, accounting for 55.5\% of all discovered bugs across all provers. This finding has important implications for Sledgehammer users: when automated proof search fails, the most likely cause is a prover timeout rather than an actual error or crash. This suggests that increasing timeout thresholds or implementing more sophisticated timeout management could significantly improve Sledgehammer's success rate.

All three major provers exhibit substantial performance problems, though with different characteristics. E Prover shows a relatively balanced distribution across timeout, error, and slowdown categories, suggesting diverse performance issues. cvc5 demonstrates a higher proportion of timeout bugs (58.0\%), indicating potential strategy selection issues or incompleteness in certain logical domains. Z3 shows the highest timeout ratio (70.4\%) but the lowest overall bug count, suggesting it is generally more reliable but occasionally encounters difficult problems. These bugs directly impact Sledgehammer's usability: when a user invokes Sledgehammer and it fails to find a proof, the underlying cause is often a prover timeout or error rather than an unprovable goal.

\subsubsection{Integration Fuzzing Results}

\begin{table}[h]
\centering
\caption{Integration Fuzzing Campaign Results (Mirabelle Validation)}
\label{tab:fuzzing}
\begin{tabular}{lrrr}
\toprule
\textbf{Campaign} & \textbf{Mutations} & \textbf{Integration Bugs} & \textbf{Time (min)} \\
\midrule
Round 1 & 41 & 0 & 4.1 \\
Round 2 & 173 & 0 & 21.7 \\
\midrule
\textbf{Total} & \textbf{214} & \textbf{0} & \textbf{25.8} \\
\bottomrule
\end{tabular}
\end{table}

The fuzzing campaigns demonstrated effective mutation generation and testing. Testing 214 mutations took 25.8 minutes total, with an average of 7.2 seconds per mutation for Mirabelle validation. This time includes both Sledgehammer execution and Mirabelle's validation logic. The mutation engine successfully employed 9 out of 10 available mutation operators, with high operator utilization indicating effective seed selection covering diverse Isabelle features. All 214 mutations were validated using Mirabelle, Isabelle's official testing tool, providing authoritative verdicts on integration correctness with no false positives possible by design.


\subsection{Analysis}

\subsubsection{Resilience of the Integration Layer}

The total absence of integration failures across our fuzzing campaign serves as a \textbf{stress test} of Isabelle's architectural maturity. This result, while potentially surprising, is itself a scientifically significant empirical finding that warrants careful interpretation. We consider several complementary explanations.

Unlike the underlying provers, which we found to be susceptible to performance degradation (see Section 5.2.1), the Sledgehammer interface correctly handled all AST-level mutations. The most straightforward explanation is that Isabelle's Sledgehammer integration layer exhibits high stability \textbf{within our tested mutation space}. The Sledgehammer interface has been under active development and use for over a decade, during which time it has been extensively tested both manually and through continuous integration in the Isabelle development process. The interface handles a substantial volume of proof attempts in practice, and many obvious bugs are likely to have been discovered and fixed through this extensive real-world testing. This suggests that the community's engineering efforts on the translation and reconstruction layers have achieved a high degree of correctness, effectively shielding users from internal logic errors even under adversarial input conditions.

Our validation methodology using Mirabelle (Isabelle's official testing tool) provides confidence in this interpretation. All 214 mutations were validated directly by Mirabelle, ensuring authoritative results. The absence of integration bugs across all tested mutations, while limited to the tested mutation space, suggests robust integration layer implementation within the scope of our testing.

The contrast between 519 prover performance bugs and the absence of integration bugs in our tested mutation space provides insights into the Sledgehammer ecosystem. Within the scope of our testing, reliability issues appear to stem more from the underlying automated theorem provers than from the integration layer. This observation, while limited to our test set, suggests directions for quality assurance efforts. For Sledgehammer users experiencing proof automation failures, prover limitations (timeouts, errors, or inability to find proofs) may be more common causes than integration layer bugs, though this requires further investigation beyond our tested mutation space.

\subsubsection{Sledgehammer Capability Boundaries}

Testing with \texttt{Extreme\_Cases.thy} revealed three interesting timeout cases where Sledgehammer exceeded the 30-second threshold on lemmas that native Isabelle tactics proved immediately. These cases expose practical limitations in Sledgehammer's ability to handle certain proof patterns.

One timeout occurred on a lemma about mutually recursive \texttt{even} and \texttt{odd} functions:

\begin{lstlisting}[language=isabelle, caption=Mutual Recursion Pattern (Extreme\_Cases.thy)]
fun even :: "nat => bool" and odd :: "nat => bool" where
  "even 0 = True" |
  "even (Suc n) = odd n" |
  "odd 0 = False" |
  "odd (Suc n) = even n"

lemma even_or_odd: "even n \<or> odd n"
  by (induction n) auto  (* Native proof succeeds *)
  (* sledgehammer times out after 30s *)
\end{lstlisting}

Sledgehammer timed out after 30 seconds, yet the proof was trivial using \texttt{by (induction n) auto}. The difficulty appears to stem from encoding mutual recursion into TPTP format---external provers lack native support for mutually defined functions and must work with less natural encodings that increase search complexity.

Another timeout involved a Fibonacci lemma requiring custom induction:

\begin{lstlisting}[language=isabelle, caption=Custom Induction Pattern (Extreme\_Cases.thy)]
function fib :: "nat => nat" where
  "fib 0 = 0" |
  "fib (Suc 0) = 1" |
  "fib (Suc (Suc n)) = fib (Suc n) + fib n"
  by pat_completeness auto
termination by (relation "measure id") auto

lemma fib_positive: "n > 0 ==> fib n > 0"
  by (induction n rule: fib.induct) auto  (* Native proof succeeds *)
  (* sledgehammer times out *)
\end{lstlisting}

Again, Sledgehammer failed while native tactics succeeded immediately. This suggests that user-defined induction principles, which are common in realistic Isabelle developments, do not map cleanly to the first-order reasoning that external provers employ. Similar issues arose with complex nested set operations involving multiple quantifiers and structural recursion.

These timeouts are not integration bugs in the technical sense---Sledgehammer correctly reports that no proof was found within the time limit and returns control to the user. However, they reveal where Sledgehammer's automation falls short. Users working with mutual recursion, custom induction schemes, or complex nested operations should expect Sledgehammer to struggle and may save time by immediately using native tactics. For tool developers, these patterns suggest specific encoding improvements: better support for recursive definitions in TPTP translation, or hybrid strategies that recognize when problems are better suited to native tactics than external provers.

\subsubsection{Prover Bug Impact}

Although the 519 discovered bugs reside in the external provers rather than Sledgehammer's integration layer, they have direct and significant impact on Sledgehammer's effectiveness as a proof automation tool. Understanding this impact requires recognizing that Sledgehammer's value to users depends entirely on its ability to find proofs, which in turn depends on prover reliability and performance.

The usability impact is most immediately visible through timeout bugs, which constitute 288 of our 519 discoveries. When a user invokes Sledgehammer on a lemma and waits for automated proof discovery, a prover timeout means Sledgehammer cannot assist, forcing the user to construct the proof manually. Even though the lemma might be provable (as evidenced by other provers succeeding), the timeout in one prover reduces the overall success probability of Sledgehammer's multi-prover strategy. Given that Sledgehammer typically runs multiple provers in parallel and returns the first successful result, timeout bugs directly reduce the probability of proof discovery and increase the time users spend on manual proof construction.

The reliability impact manifests through the 115 error bugs we discovered. When a prover crashes or returns an error, it not only fails to find a proof but may also cause Sledgehammer to report failure even when other provers might have succeeded. Depending on Sledgehammer's error handling strategy, a prover error might trigger early termination of the proof search or cause confusing error messages to be presented to the user. In the worst case, errors could indicate soundness issues in the prover, though we have no evidence of such issues in our testing.

The performance impact accumulates from the 116 slowdown bugs, where provers succeed but take dramatically longer than alternatives. While these cases do not represent complete failures, they degrade the user experience by increasing the latency of proof automation. In interactive proof development, where users may invoke Sledgehammer hundreds of times during a session, even seemingly small delays accumulate into substantial productivity losses. Furthermore, in batch processing scenarios such as continuous integration testing of large proof developments, prover slowdowns can make the difference between feasible and infeasible testing times.

Collectively, these findings are valuable to multiple audiences. For Isabelle developers, the bug distribution informs prover selection and configuration decisions---for instance, our data suggests that Z3, despite having fewer total bugs, might be a more reliable default choice for certain problem types. For prover developers, the 519 bug reports provide concrete test cases for performance optimization and regression testing. For Sledgehammer users, our findings explain why proof automation sometimes fails and suggest mitigation strategies such as adjusting timeout settings or manually selecting different prover combinations.

\section{Discussion}

\subsection{Contributions to Testing Methodology}

Our work makes several contributions to fuzzing and testing methodology that extend beyond the specific domain of theorem prover testing. Our approach demonstrates the value of leveraging official validation tools directly for bug detection. Rather than developing custom oracles that require training and validation, we use Mirabelle (Isabelle's official testing tool) directly, ensuring results are authoritative by design. This methodology is applicable in domains where official validation tools exist and can be integrated into automated testing workflows.

The techniques we employed for bug detection represent practical insights for testing complex systems. Our approach demonstrates that leveraging official validation tools directly can simplify testing workflows while ensuring result accuracy. In Isabelle's case, Mirabelle provides built-in logic for distinguishing theory-level errors from integration bugs, eliminating the need for custom classification logic. This separation of concerns---where theory errors (syntax, type issues) are correctly identified as distinct from integration failures (Sledgehammer crashes, TPTP errors)---is critical for accurate bug detection in mutation-based testing.

\subsection{Practical Impact}

Our work provides immediate practical value to multiple stakeholder communities. 

\textbf{For Isabelle developers and maintainers}, our testing campaign provides \textbf{fuzzing-based evidence} that the integration layer is stable \textbf{within the tested mutation space}. The fact that all 214 diverse mutations were handled without integration failures, validated by official Mirabelle testing, gives confidence that the interface handles these tested edge cases correctly. Additionally, our identification of problematic prover configurations (specifically, which provers timeout or error on which types of problems) can inform default prover selection strategies in future Isabelle releases. Perhaps most valuably, our framework provides a ready-made regression testing infrastructure that can be deployed whenever Sledgehammer's encoding strategies or integration logic are modified, ensuring that changes do not introduce new bugs.

\textbf{For prover developers}, particularly those maintaining E Prover, cvc5, and Z3, our work delivers 519 concrete bug reports documenting specific performance issues. Each bug report includes the triggering TPTP problem, the observed behavior (timeout, error, or slowdown), and comparison with other provers' behaviors on the same problem. Analysis of these bugs reveals patterns of problematic problems---for instance, certain quantifier structures or arithmetic reasoning patterns that consistently trigger timeouts in E Prover but not in cvc5. These patterns represent concrete opportunities for optimization and strategy improvement that could enhance prover performance on realistic theorem proving workloads.

\textbf{For the broader research community}, our work contributes both methodology and empirical insights. Our approach demonstrates the value of leveraging official validation tools directly in automated testing workflows. Our AST-based mutation strategies for logical formulas represent a novel application of grammar-aware fuzzing to theorem proving, and could be adapted to other formal methods tools. The empirical finding that Sledgehammer's integration is stable (0 bugs across 214 mutations) while underlying provers have numerous performance issues (519 bugs) challenges assumptions about where reliability problems occur in proof assistant ecosystems.

\subsection{Limitations and Future Work}

Several limitations of our current work suggest promising directions for future research.

\textbf{Mutation Operators}: Our mutation operators, while diverse, focus primarily on syntactic and logical transformations that change the structure or operators in formulas. Future work could explore semantic-preserving mutations that maintain the provability of lemmas while varying their syntactic presentation. Such mutations would test whether Sledgehammer handles semantically equivalent but syntactically different formulations consistently. Type-based mutations that specifically target Isabelle's type system could explore how Sledgehammer handles polymorphism, type classes, and type inference. Domain-specific mutation patterns derived from common proof patterns in the Archive of Formal Proofs could increase the likelihood of triggering domain-specific bugs.

\textbf{Campaign Scale}: The scale of our fuzzing campaign, while sufficient to demonstrate framework capabilities, represents only a fraction of the input space that could be explored. Our 214 mutations provide good coverage of the implemented mutation operators and seed theory diversity, but campaigns with thousands or tens of thousands of mutations could potentially reveal rarer edge cases that only manifest under unusual combinations of features. Larger-scale campaigns could explore more of the input space, though our current scale was constrained by the time required for Mirabelle validation.

\textbf{Code Coverage}: Our current evaluation lacks quantitative code coverage metrics for the Sledgehammer implementation itself. While we achieve good diversity in terms of mutation types and seed theories, we cannot quantify what fraction of Sledgehammer's code paths are actually exercised by our tests. Instrumenting Isabelle to collect coverage data during fuzzing would enable coverage-guided mutation, where mutation strategies are adapted based on which code paths remain unexplored. This feedback-driven approach, successfully employed in tools like AFL for binary fuzzing, could significantly improve the efficiency of bug discovery in Sledgehammer.

\textbf{Framework Specificity}: Our framework is currently specific to Isabelle/HOL, but the core concepts and techniques are generalizable to other proof assistants that integrate external provers. Coq's CoqHammer, Lean's automation tactics, and HOL4's Metis all face similar integration challenges. Adapting our framework to these systems would both validate the generalizability of our approach and potentially discover bugs in these other critical proof assistant tools. Such cross-system evaluation would also enable comparative analysis of different design choices in proof assistant integration layers.

\subsection{Threats to Validity}

Several factors may limit the generalizability of our findings and warrant careful consideration when interpreting results.

\textbf{Software Versions}: External validity is constrained by our specific software versions. All testing was conducted using Isabelle2025, E Prover version 3.1, cvc5 version 1.0.8, and Z3 version 4.12.2. The bugs we discovered are specific to these versions, and different versions may exhibit different behaviors. This is particularly relevant given the active development of all these tools---bug fixes, performance improvements, and new features are regularly added. Our results therefore represent a snapshot of reliability at a specific point in time rather than timeless characteristics of these tools. Replication studies using different versions would be valuable for tracking how reliability evolves over time and whether specific bugs are fixed in newer releases.

\textbf{Seed Theory Corpus}: The selection and size of our seed theory corpus presents another validity consideration. While our 10 seed theories were carefully chosen to cover diverse Isabelle features including arithmetic, lists, sets, logic, induction, functions, options, pairs, numbers, and relations, they represent only a small fraction of Isabelle's extensive feature set. Features such as locales, type classes, quotient types, transfer reasoning, and many advanced proof techniques are not covered by our current seeds. A larger and more diverse seed corpus drawn from the Archive of Formal Proofs or real-world Isabelle projects could improve feature coverage and potentially reveal bugs that only manifest with more complex Isabelle constructs.

\textbf{Timeout Threshold}: Our choice of a 30-second timeout threshold for prover testing, while informed by common practice in the automated reasoning community, is somewhat arbitrary. Different timeout settings could reveal different bug patterns: shorter timeouts would classify more cases as timeouts (potentially inflating bug counts), while longer timeouts would give provers more opportunity to succeed (potentially reducing observed timeout bugs but increasing total testing time). The appropriate timeout depends on the use case---interactive proof development might tolerate only 5-second delays, while batch verification might accept 60-second timeouts. Our results should therefore be interpreted as relative to the 30-second threshold rather than as absolute measures of prover reliability.

\textbf{Platform Specificity}: All our experiments were conducted on a single platform: macOS 26.1 running on Apple M3 Max hardware with 64GB RAM. Platform-specific differences in process management, file I/O performance, memory management, and floating-point arithmetic could potentially affect prover behavior. Linux and Windows platforms, which are more commonly used in automated reasoning research and production deployments, might exhibit different bug patterns. While we have no specific reason to believe our bugs are platform-specific, replication on multiple platforms would strengthen confidence in the generalizability of our findings.

\section{Conclusion}

This paper presents a holistic testing framework for the Isabelle Sledgehammer ecosystem, addressing both prover reliability and integration resilience through a dual approach.

This work makes four key empirical and methodological achievements. First, through rigorous differential testing of over 1000 TPTP problems across three major automated theorem provers, we discovered 519 performance bugs encompassing timeouts, errors, and slowdowns. These bugs represent concrete, reproducible failures that impact real-world users of Sledgehammer. The scale and methodical nature of this bug discovery demonstrates the effectiveness of differential testing for automated reasoning tools.

Second, we developed and deployed a complete AST-based fuzzer specifically designed for Isabelle theories, implementing 10 distinct mutation operators that target different aspects of the Sledgehammer integration. Unlike generic fuzzing tools that operate on bytes or tokens, our fuzzer understands Isabelle's grammar and can perform semantically meaningful transformations while maintaining syntactic validity. This grammar-aware approach represents an advance over naive mutation strategies and could serve as a template for fuzzing other domain-specific languages.

Third, we validated our AST-based fuzzer through systematic testing using Mirabelle, Isabelle's official testing tool. Testing 214 mutations found no integration bugs, providing evidence of Sledgehammer's integration robustness within the tested mutation space. This demonstrates the value of leveraging official validation tools directly rather than developing custom oracles.

Fourth, our mutation-based evaluation establishes a \textbf{lower bound on the reliability} of the Sledgehammer interface on the tested mutation space. By withstanding 214 valid structural mutations without a single failure---validated by Mirabelle---we provide evidence that, in our evaluation, instability appears confined to the external provers (519 bugs) rather than the integration logic. This finding helps direct future testing efforts toward prover performance rather than interface reliability.

From a methodological perspective, we contribute several generalizable techniques. Our approach demonstrates the value of integrating official validation tools directly into automated testing workflows rather than developing custom oracles. Our AST-based mutation strategies for logical formulas extend grammar-aware fuzzing to the theorem proving domain and could be adapted to other formal methods tools. The fuzzer design provides a template that other researchers can follow when developing mutation-based testing frameworks for systems where official testing tools exist.

The practical impact of our work extends to both users and developers. Sledgehammer users can have confidence that integration bugs are unlikely to be the source of proof automation failures; performance issues with underlying provers are the more probable cause. Isabelle developers gain both reassurance about interface stability and concrete prover performance data that can inform prover selection and configuration decisions. Prover developers receive detailed bug reports that can guide optimization efforts. The broader formal methods community gains a reusable testing framework and a demonstration that methodical testing can yield significant bug discoveries while maintaining scientific rigor.

Future work should pursue several directions. Extended fuzzing campaigns with thousands of mutations could reveal rarer edge cases. Adaptation of our framework to other proof assistants such as Coq, Lean, or HOL4 would test the generalizability of our approach. Coverage-guided mutation, where mutation strategies are informed by code coverage data from Sledgehammer, could improve bug-finding efficiency. Investigation of semantic-preserving mutations that maintain lemma provability while varying syntactic structure could test different aspects of the integration. Finally, longitudinal studies tracking bug discovery across Isabelle and prover versions could provide insights into software evolution and regression patterns in formal methods tools.

This work demonstrates that rigorous testing of proof assistant ecosystems can yield significant bug discoveries while maintaining scientific rigor through official tool validation. The 519 prover bugs and observed Sledgehammer stability on the tested mutations represent concrete contributions to formal verification reliability.

\section*{Acknowledgments}

We thank Dr. Mohammad Ahmad Abdulaziz Ali Mansour and Dr. Karine Even Mendoza for supervision and guidance. This work was supported by the Knowledge Exchange Projects program at King's College London in collaboration with Amazon.

\section*{Data Availability}

Source code, test data, and bug reports are available at the project repository. All experiments are fully reproducible following the provided documentation.

\begin{thebibliography}{99}

\bibitem{afp}
Archive of Formal Proofs. (n.d.). \textit{Archive of Formal Proofs (AFP)}. \href{https://www.isa-afp.org/}{\texttt{https://www.isa-afp.org/}}

\bibitem{afl}
Zalewski, M. (n.d.). \textit{American Fuzzy Lop (AFL) technical details}. \url{https://lcamtuf.coredump.cx/afl/technical_details.txt}

\bibitem{barbosa2022cvc5}
Barbosa, H., Barrett, C. W., Brain, M., Kremer, G., Lachnitt, H., Mann, M., Mohamed, A., Mohamed, M., Niemetz, A., N\"otzli, A., Ozdemir, A., Preiner, M., Reynolds, A., Sheng, Y., Tinelli, C., \& Zohar, Y. (2022). cvc5: A versatile and industrial-strength SMT solver. In \textit{TACAS 2022: Tools and Algorithms for the Construction and Analysis of Systems}, Lecture Notes in Computer Science, Vol. 13243, pp. 415--442. Springer. \url{https://doi.org/10.1007/978-3-030-99524-9_24}

\bibitem{brummayer2009}
Brummayer, R., \& Biere, A. (2009). Fuzzing and delta-debugging SMT solvers. In \textit{International Workshop on Satisfiability Modulo Theories} (SMT), pp. 1--5.

\bibitem{klein2009sel4}
Klein, G., Elphinstone, K., Heiser, G., Andronick, J., Cock, D., Derrin, P., \ldots \& Winwood, S. (2009). seL4: Formal verification of an OS kernel. \textit{Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles} (pp. 207--220).

\bibitem{libfuzzer}
LLVM Project. (n.d.). \textit{LibFuzzer}. \url{https://llvm.org/docs/LibFuzzer.html}

\bibitem{oddfuzz2023}
Cao, S., He, B., Sun, X., Ouyang, Y., Zhang, C., Wu, X., Su, T., Bo, L., Li, B., Ma, C., Li, J., \& Wei, T. (2023). \textit{ODDFUZZ: Discovering Java deserialization vulnerabilities via structure-aware directed greybox fuzzing}. \href{https://arxiv.org/abs/2304.04233}{arXiv:2304.04233}.

\bibitem{paulson2012sledgehammer}
Paulson, L. C., \& Blanchette, J. C. (2012). Three years of experience with Sledgehammer, a practical link between automatic and interactive theorem provers. \textit{Journal of Automated Reasoning}, 49(3), 389--405.

\bibitem{squirrel2020}
Zhong, R., Chen, Y., Hu, H., Zhang, H., Lee, W., \& Wu, D. (2020). \textit{SQUIRREL: Testing database management systems with language validity and coverage feedback}. In \textit{Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (CCS '20)} (pp. 955--970). ACM. \url{https://doi.org/10.1145/3372297.3417260}

\bibitem{yang2011}
Yang, X., Chen, Y., Eide, E., \& Regehr, J. (2011). Finding and understanding bugs in C compilers. \textit{ACM SIGPLAN Notices}, 46(6), 283--294.

\bibitem{schulz2013}
Schulz, S. (2013). System description: E 1.8. In \textit{LPAR-19: Logic for Programming, Artificial Intelligence, and Reasoning}, pp. 735--743. Springer.

\bibitem{demoura2008}
de Moura, L., \& Bj\o rner, N. (2008). Z3: An efficient SMT solver. In \textit{TACAS 2008: Tools and Algorithms for the Construction and Analysis of Systems}, Lecture Notes in Computer Science, Vol. 4963, pp. 337--340. Springer.

\bibitem{nipkow2002}
Nipkow, T., Paulson, L. C., \& Wenzel, M. (2002). \textit{Isabelle/HOL --- A Proof Assistant for Higher-Order Logic}. Springer.

\bibitem{mirabelle}
Bulwahn, L., \& Krauss, A. (2011). Mirabelle: Automatic testing of automated reasoning tools. In \textit{PAAR 2011: Workshop on Practical Aspects of Automated Reasoning}, pp. 1--5.

\end{thebibliography}

\end{document}

