\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{url}
% Prevent URL line breaks
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
\Urlmuskip=0mu plus 1mu
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes}
\usepackage{algorithm2e}
\usepackage{enumitem}

% Page setup - narrow margins
\geometry{left=0.75in,right=0.75in,top=0.75in,bottom=0.75in}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

% Disable hyphenation completely - words that don't fit go to next line
\hyphenpenalty=10000
\exhyphenpenalty=10000
\doublehyphendemerits=10000
\finalhyphendemerits=10000
\adjdemerits=10000
% Allow more flexible line spacing to avoid hyphenation
\tolerance=9999
\emergencystretch=10em
\hbadness=10000
\hfuzz=10pt
\sloppy

% Code listings setup - Isabelle language with symbol mapping
\lstdefinelanguage{isabelle}{
    basicstyle=\ttfamily\small,
    columns=fullflexible,
    keepspaces=true,
    mathescape=true,
    numbers=none,  % Disable line numbers to avoid confusion with breaklines
    literate=
        {\\<forall>}{{$\forall$}}1
        {\\<exists>}{{$\exists$}}1
        {\\<Rightarrow>}{{$\Rightarrow$}}1
        {\\<Longrightarrow>}{{$\Longrightarrow$}}1
        {==>}{{$\Rightarrow$}}3
        {=>}{{$\Rightarrow$}}2
        {\\<or>}{{$\lor$}}1
        {\\<and>}{{$\land$}}1
        {\\<longrightarrow>}{{$\longrightarrow$}}1
        {\\<noteq>}{{$\neq$}}1,
    commentstyle=\color{gray}\itshape,
    showspaces=false,
    showstringspaces=false,
    tabsize=2
}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=Python,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    showspaces=false,
    showstringspaces=false,
    tabsize=2
}

% Title information
\title{Testing the Isabelle Sledgehammer Ecosystem: \\
A Dual Approach to Prover and Integration Fuzzing}
\author{Qilan Lin (K21204786) \\
        Department of Informatics, \\
        King's College London \\
        \texttt{qilan.lin@kcl.ac.uk}}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
Proof assistants like Isabelle/HOL rely on complex ecosystems involving external automated theorem provers (ATPs) and SMT solvers integrated through the Sledgehammer interface. While the interface layer has received significant engineering attention, the reliability of the underlying provers themselves directly impacts Sledgehammer's usability. This paper presents a holistic testing framework that addresses both prover reliability and interface robustness through a dual testing approach.

Our framework makes two key contributions: (1) a \textit{differential testing methodology} that discovered 519 performance anomalies (timeouts, errors, and severe slowdowns) across three major provers (E Prover, cvc5, Z3); and (2) an \textit{AST-based mutation fuzzer} for Isabelle theories with 10 intelligent mutation operators specifically designed for testing Sledgehammer integration, validated using Mirabelle as the authoritative testing tool.

Experimental evaluation on 204 mutations validated by Mirabelle (Isabelle's official testing tool) with comprehensive hidden exception detection found 0 integration failures and 0 logged hidden exceptions within our tested mutation space. Additionally, aggressive stress testing specifically targeting proof reconstruction---including 63 edge-case tests covering type breaking, encoding attacks, extreme values, and boundary conditions---found 0 reconstruction bugs. In contrast, differential testing exposed substantial volatility in underlying provers (519 performance anomalies).

\textbf{Keywords:} Fuzzing, Isabelle/HOL, Sledgehammer, Differential Testing, AST Mutation, Test Oracles, Theorem Provers
\end{abstract}

\section{Introduction}

Proof assistants such as Isabelle/HOL play a critical role in formal verification, enabling the verification of safety-critical systems from operating system kernels (Klein et al., 2009) to cryptographic protocols. Isabelle's Sledgehammer tool (Paulson \& Blanchette, 2012) significantly enhances productivity by automatically invoking external automated theorem provers (ATPs) and SMT solvers to find proofs. However, the reliability of this entire ecosystem---both the integration layer and the underlying provers---is crucial for trustworthy verification.

\subsection{Motivation}

The Sledgehammer ecosystem presents two distinct reliability challenges:

\textbf{Prover Reliability}: External provers (E Prover, cvc5, Z3, Vampire) form the computational foundation of Sledgehammer. Performance issues in these provers---timeouts, crashes, or errors---directly impact Sledgehammer's ability to find proofs. If a prover times out or fails on a provable problem, Sledgehammer cannot assist the user, reducing proof automation effectiveness.

\textbf{Integration Reliability}: The interface layer between Isabelle and external provers involves TPTP/SMT-LIB encoding, prover invocation, output parsing, and proof reconstruction. Bugs at this layer can cause crashes, incorrect results, or reconstruction failures even when provers function correctly.

While traditional fuzzing has proven effective for general software testing, existing approaches have two key limitations for proof assistant testing:

\begin{enumerate}
    \item \textbf{Lack of prover-specific testing}: General fuzzing tools treat provers as black boxes, missing performance degradation patterns that matter for proof automation.
    \item \textbf{High false positive rates}: Naive bug detection may misclassify warnings as errors or theory-level issues as integration bugs.
\end{enumerate}

\subsection{Contributions}

This paper presents an extensive testing framework that addresses both challenges through a methodical dual approach combining prover reliability testing with integration robustness evaluation.

We developed a differential testing methodology specifically designed for automated theorem prover performance evaluation. By rigorously comparing the behavior of three major provers (E Prover, cvc5, and Z3) across over 1000 TPTP problems, we identified 519 performance anomalies (potential performance bugs) that directly impact Sledgehammer's usability. These anomalies are distributed across E Prover (349 cases, representing 67.2\% of the total), cvc5 (143 cases, 27.6\%), and Z3 (27 cases, 5.2\%). The anomalies are categorized by type into timeouts (288 cases, 55.5\% of total), errors (115 cases, 22.2\%), and slowdowns (116 cases, 22.3\%). We report a large-scale, reproducible cross-prover performance-discrepancy campaign in a Sledgehammer-relevant ATP/SMT setting, providing concrete evidence of widespread performance issues that affect real-world proof automation.

We also developed a complete AST-based fuzzer specifically designed for testing Sledgehammer integration through Isabelle theory mutation. Unlike existing fuzzing tools that operate on bytes or tokens without understanding structure, our fuzzer parses Isabelle theories to extract lemmas and their components, then applies grammar-aware transformations that maintain syntactic validity. The fuzzer implements ten distinct mutation operators targeting different aspects of the integration layer, including quantifier transformations, logical operator modifications, term reordering, and proof method variations. We generated and tested 204 mutations across 11 carefully selected seed theories, demonstrating that our fuzzer can methodically explore the input space while maintaining syntactic validity.

To ensure accurate bug detection, we use Mirabelle (Isabelle's official testing tool for Sledgehammer) as the validator. Mirabelle provides an official, tool-aligned oracle for classifying Sledgehammer outcomes, substantially reducing misclassification risk compared with ad-hoc log parsing. Mirabelle directly determines whether mutations reveal integration failures by examining Sledgehammer's execution, distinguishing integration failures from theory-level errors.

Finally, our \textbf{systematic testing campaign} provides empirical evidence about Sledgehammer's integration layer \textbf{within the tested mutation space}. Testing over 204 diverse mutations validated by Mirabelle with comprehensive instrumentation for hidden exception detection observed \textbf{no integration failures}. We further validated these findings through aggressive stress testing with 63 additional edge-case tests specifically targeting proof reconstruction vulnerabilities, including type breaking attacks, encoding manipulation, extreme values, and boundary conditions---all of which Sledgehammer handled correctly. This stands in stark contrast to the backend provers (519 performance anomalies), suggesting reliability concerns may be more concentrated in the computational layer than in the interface implementation within our test scope.

\section{Background and Related Work}

\subsection{Isabelle Sledgehammer}

Isabelle Sledgehammer (Paulson \& Blanchette, 2012) automates proof discovery by:
\begin{enumerate}
    \item Selecting relevant facts from the proof context
    \item Encoding HOL formulas into TPTP or SMT-LIB
    \item Invoking external provers (E, Vampire, Z3, cvc5, etc.)
    \item Parsing prover responses
    \item Reconstructing proofs using Metis or SMT replay
\end{enumerate}

Our testing targets both the underlying provers (steps 3) and the integration layer (steps 2, 4, 5).

\subsection{Fuzzing and Differential Testing}

\textbf{Coverage-Guided Fuzzing}: AFL (AFL Technical Details, n.d.) and LibFuzzer (LLVM Project, n.d.) use coverage feedback to guide mutation, effective for finding crashes but not semantic bugs.

\textbf{Differential Testing}: Used successfully for compilers (Yang et al., 2011) and SMT solvers (Brummayer \& Biere, 2009), comparing outputs from different implementations to detect bugs. Our differential approach extends this to theorem prover performance testing.

\textbf{Structure-Aware Fuzzing}: ODDFUZZ (2023) for Java deserialization and SQUIRREL (2020) for databases demonstrate benefits of grammar-aware mutation. Our AST-based approach applies this to logical formulas.

\subsection{Gap Analysis}

\begin{table}[h]
\centering
\caption{Comparison with Existing Approaches}
\label{tab:comparison}
\scalebox{0.85}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Prover Testing} & \textbf{Integration Testing} & \textbf{False Positive Control} & \textbf{Official Validation} \\
\midrule
AFL/LibFuzzer & Limited & No & No & No \\
SMT-LIB Fuzzers & Yes & No & Limited & No \\
Manual Testing & Ad-hoc & Ad-hoc & N/A & Sometimes \\
\textbf{This Work} & \textbf{Systematic (519 anomalies)} & \textbf{Yes (204 tests)} & \textbf{Yes (Mirabelle)} & \textbf{Yes (Official Tool)} \\
\bottomrule
\end{tabular}%
}
\end{table}

As Table~\ref{tab:comparison} illustrates, prior approaches address individual aspects but none provide a unified framework that methodically tests prover performance at scale, specifically targets integration layer reliability through fuzzing, rigorously eliminates false positives, and validates findings using official tools.

\section{Methodology}

\subsection{Part A: Prover Differential Testing}

\subsubsection{Test Suite}

Our prover testing methodology relies on an extensive test suite comprising over 1000 problems drawn from diverse domains. These problems span fundamental areas including arithmetic reasoning, set theory operations, quantified formulas, and real-world verification scenarios. The diversity of this test suite is crucial for exposing performance issues across different problem characteristics, as prover performance often varies considerably depending on the logical structure, quantifier complexity, and domain-specific features of the input problem.

The selection of TPTP as our canonical benchmark source is deliberate: TPTP (Thousands of Problems for Theorem Provers) is the de facto standard interchange format for first-order logic problems in the automated reasoning community. Each problem is translated into the prover's native input format (TPTP for E Prover; SMT-LIB for cvc5 and Z3) before execution. We use Isabelle's built-in TPTP-to-SMT-LIB conversion facilities (via Sledgehammer's encoding pipeline) to ensure semantic equivalence between formats. Our test suite primarily comprises FOF (First-Order Formulas) and CNF (Clause Normal Form) problems from TPTP, which map cleanly to SMT-LIB's uninterpreted functions and quantifiers. While format conversion introduces potential encoding differences, we mitigate this concern by: (1) using Isabelle's standard conversion pipeline that is already production-tested in Sledgehammer, (2) comparing only semantically equivalent problems where multiple provers succeed, and (3) focusing on timeout/error discrepancies rather than fine-grained timing differences. This approach preserves comparability while respecting tool interfaces and facilitates reproducibility with other testing efforts.

\subsubsection{Differential Oracle}

Our differential testing oracle operates by executing each problem across three major provers---E Prover, cvc5, and Z3---and methodically comparing their behaviors to detect performance anomalies. For timeout and error detection, we run all three provers concurrently with identical timeout settings (30 seconds) and computational resources. For slowdown detection, we execute provers sequentially to avoid resource contention and timing noise from CPU scheduling; each prover runs with CPU affinity binding and we repeat measurements three times, reporting the median execution time to minimize variance. This sequential approach ensures that slowdown anomalies reflect genuine algorithmic differences rather than concurrent execution artifacts. All problems are executed using native TPTP format for E Prover and SMT-LIB translations (via Isabelle's conversion) for the SMT solvers. The results are then analyzed to identify discrepancies that indicate potential performance anomalies.

\begin{algorithm}[H]
\caption{Differential Testing Oracle}
\label{alg:differential}
\SetAlgoLined
\KwData{TPTP problem $p$, provers $\{E, cvc5, Z3\}$, timeout $t$}
\KwResult{Bug report or PASS}
\For{each prover $P$ in $\{E, cvc5, Z3\}$}{
    $result_P \gets \text{RunProver}(P, p, t)$\;
}
\If{any prover timed out while others succeeded}{
    \Return TIMEOUT\_ANOMALY($p$, prover)\;
}
\If{any prover errored while others succeeded}{
    \Return ERROR\_ANOMALY($p$, prover)\;
}
\If{execution times vary significantly (sequential measurement)}{
    \Return SLOWDOWN\_ANOMALY($p$, prover)\;
}
\Return PASS\;
\end{algorithm}

The differential oracle classifies anomalies into three distinct categories based on the nature of the performance discrepancy observed. \textbf{Timeout anomalies} occur when a prover exceeds the configured timeout threshold of 30 seconds while at least one of the other provers successfully completes the same problem. This pattern indicates that the problem is provable (as demonstrated by the successful prover), but the timing-out prover either employs an inefficient strategy or encounters a performance regression. Such anomalies are particularly problematic for Sledgehammer users, as they prevent automated proof discovery even when a proof exists. We note that timeout discrepancies may reflect algorithmic differences, strategy selection, or resource allocation choices rather than implementation bugs; hence we classify them as performance anomalies requiring further investigation.

\textbf{Error cases} manifest when a prover returns a non-zero exit code or produces error messages while other provers succeed on the same problem. These errors may stem from parser failures, internal assertion violations, format conversion issues, or unhandled edge cases in the prover's logic. Unlike timeouts, which may reflect strategy differences, errors represent actual failures in the prover's execution that could potentially indicate deeper issues such as implementation bugs. Error cases are the most likely to represent confirmed bugs after upstream evaluation, as they indicate explicit failures rather than performance variations.

\textbf{Slowdown anomalies} represent cases where a prover successfully solves the problem but takes significantly longer than alternative provers. We define ``significantly longer'' as execution time exceeding 10 times the median execution time of successful provers on the same problem. To ensure accuracy, slowdown measurements are performed sequentially (not concurrently) with CPU affinity binding, and each problem is executed three times per prover; we report the median execution time to minimize variance from system noise. Slowdown anomalies are reported only when at least two provers succeed; the reference median is computed over the \textit{other} successful provers excluding the target prover. While these cases do not represent complete failures, they indicate suboptimal performance that degrades user experience and reduces the efficiency of automated proof search in Sledgehammer.

In this paper, we use ``performance anomaly'' to denote a reproducible cross-prover discrepancy (timeout, error, or $\geq$10$\times$ slowdown) under a fixed resource budget; this does not necessarily imply a confirmed implementation defect.

\subsection{Part B: Integration Fuzzing}

\subsubsection{AST-Based Mutation}

Unlike TPTP-level mutation, we mutate Isabelle theories directly at the AST level:

\begin{algorithm}[H]
\caption{Isabelle Theory Mutation}
\label{alg:mutation}
\SetAlgoLined
\KwData{Seed theory $T$, mutation count $n$}
\KwResult{List of mutants $M$}
$M \gets \emptyset$\;
$lemmas \gets \text{ExtractLemmas}(T)$\;
\For{$i = 1$ to $n$}{
    $lemma \gets \text{RandomChoice}(lemmas)$\;
    $type \gets \text{RandomChoice}(\text{MutationTypes})$\;
    $mutant \gets \text{ApplyMutation}(T, lemma, type)$\;
    \If{$mutant \neq T$ and $mutant \notin M$}{
        $M \gets M \cup \{mutant\}$\;
    }
}
\Return $M$\;
\end{algorithm}

Our mutation engine implements ten distinct mutation operators designed to test different aspects of the Sledgehammer integration layer. These operators can be grouped into several categories based on the types of transformations they perform.

\begin{lstlisting}[language=isabelle, caption=Example Mutations Across Different Categories]
(* Logical structure mutation: Quantifier flipping *)
lemma original_1: "(\<forall>x. P x) ==> Q"
lemma mutated_1: "(\<exists>x. P x) ==> Q"  (* Forall to Exists *)

(* Term-level mutation: Argument reordering *)
lemma original_2: "f x y = g a b"
lemma mutated_2: "f y x = g b a"  (* Test commutativity *)

(* Proof-level mutation: Explicit Sledgehammer call *)
lemma original_3: "P ==> Q" by auto
lemma mutated_3: "P ==> Q" 
  by (sledgehammer)  (* Test direct invocation *)
\end{lstlisting}

Logical structure mutations target quantifiers and connectives, testing how Sledgehammer encodes different logical constructs into TPTP format. Operators in this category include quantifier flipping (exchanging universal and existential quantifiers), formula negation (applying logical negation to change lemma polarity), and conjunction swapping (exchanging AND and OR operators). These transformations typically change lemma semantics substantially, revealing potential issues in how Sledgehammer handles different quantifier types and logical connectives during encoding and proof search.

Term-level mutations focus on function arguments and constants, exploring how Sledgehammer's term encoding interacts with prover behavior. These include term reordering (swapping function arguments to test sensitivity to argument order), identity addition (inserting semantically neutral operations like adding zero), and constant replacement (substituting constants with different values, often invalidating lemmas to test error handling). These mutations test whether Sledgehammer and external provers can correctly handle semantically equivalent but syntactically different formulas, as well as how they respond to invalid inputs.

Proof-level mutations modify proof tactics and structure, testing Sledgehammer's interaction with Isabelle's proof engines. This category includes proof method changes (varying the proof tactic used), explicit Sledgehammer invocation (directly calling Sledgehammer in proof scripts), lemma duplication (creating redundant lemmas to test namespace handling), and assumption addition (introducing additional hypotheses). These operators explore how Sledgehammer behaves when called explicitly versus implicitly, and how it handles varying proof structures and lemma dependencies.

Each operator targets specific potential failure modes in the integration layer, from TPTP encoding errors to proof reconstruction issues, enabling thorough testing of Sledgehammer's resilience across diverse input patterns.

\subsection{Part C: Mirabelle Validation}

For integration bug detection, we use Mirabelle (Bulwahn \& Krauss, 2011), Isabelle's official testing tool specifically designed for evaluating automated proof tools like Sledgehammer. Mirabelle provides authoritative judgments on integration correctness by executing theories through Sledgehammer and detecting failures in the integration layer. As an official tool maintained by the Isabelle team, Mirabelle's verdicts serve as ground truth for integration bug detection, eliminating concerns about false positives or misclassification that could arise from custom detection logic.

\subsection{Part D: Aggressive Proof Reconstruction Testing}

To complement AST-based mutation fuzzing, we developed aggressive stress testing specifically targeting Sledgehammer's proof reconstruction mechanism---the critical component that translates external prover outputs back into Isabelle-verifiable proofs. This testing employs seven distinct attack strategies:

\begin{enumerate}[noitemsep]
    \item \textbf{Type Breaking}: Mutations that create type inconsistencies (e.g., mixing \texttt{nat} and \texttt{int}, using \texttt{bool} in arithmetic contexts) to test type encoding robustness.
    \item \textbf{Encoding Attacks}: Injection of TPTP/SMT-LIB special characters, reserved words, and escape sequences to test encoding pipeline resilience.
    \item \textbf{Extreme Values}: Tests with large numbers ($10^{100}$), deeply nested expressions (15+ levels), and identifiers exceeding 500 characters.
    \item \textbf{Unicode Injection}: Mathematical symbols, Greek letters, subscripts, and special Unicode operators.
    \item \textbf{Boundary Conditions}: Empty structures, singletons, recursive base cases, and zero/identity values.
    \item \textbf{Proof Corruption}: Invalid fact references, incorrect proof method arguments, and malformed proof steps.
    \item \textbf{Configuration Fuzzing}: Non-standard Sledgehammer configurations including extreme timeouts, single-prover restrictions, and unusual type encodings.
\end{enumerate}

Additionally, we implemented Mirabelle-based stress testing that runs actual Sledgehammer proofs on complex logical formulas (nested quantifiers, higher-order functions, type class constraints) and verifies successful proof reconstruction. This dual approach---both attacking the reconstruction pipeline directly and validating real proof attempts---provides comprehensive coverage of potential reconstruction failures.

\section{Implementation}

\subsection{Technology Stack}

Our framework is implemented in Python 3.13, comprising over 8000 lines of carefully engineered code organized into modular components. We chose Python for its excellent subprocess management capabilities (essential for invoking external provers and Isabelle), rich ecosystem of testing libraries, and clear syntax that facilitates rapid prototyping and iterative development.

The codebase is organized around core components that handle different aspects of the testing workflow. For differential testing, the crash and timeout detection oracle monitors prover execution for abnormal termination and timeout violations, while the differential oracle performs multi-prover comparison by parsing and normalizing outputs from different provers to detect result inconsistencies. The theory mutation engine parses Isabelle theory files and applies mutation operators while tracking validity, enabling structured exploration of the input space. For integration testing, we integrate directly with Mirabelle for bug detection and validation.

The fuzzing campaign orchestrator coordinates the complete automated workflow, managing mutation generation, test execution, and statistical reporting. Integration bug detection is handled directly by Mirabelle, Isabelle's official testing tool, which provides authoritative validation of Sledgehammer's behavior on each mutated theory.

To ensure comprehensive bug detection beyond surface-level validation, we instrumented Sledgehammer's source code to detect hidden exceptions. By analyzing Sledgehammer's exception handling architecture, we identified that internal exceptions might be silently caught by try-catch blocks and converted to SH\_Unknown status, making them invisible to external testing. We added logging instrumentation to Sledgehammer's core exception handlers in \texttt{sledgehammer.ML}, enabling detection of exceptions that occur before they are silently converted. This instrumentation logs all exceptions caught by Sledgehammer's defensive exception handling mechanism to a dedicated log file, which our testing framework monitors for hidden bugs. This approach complements Mirabelle's validation by detecting integration bugs that might be masked by defensive programming practices.

The implementation follows software engineering best practices including comprehensive error handling, input validation, and modular design. Detailed code quality metrics and testing infrastructure documentation are available in the project repository.

\subsection{Quality Assurance}

Ensuring the reliability of a testing framework is paramount, as issues in the testing tool itself can lead to missed anomalies or false alarms. Our implementation incorporates rigorous error handling, input validation, security considerations, and resource management. Specific details including exception hierarchies, validation strategies, and code quality metrics are documented in the project repository.

\section{Experimental Evaluation}

\subsection{Experimental Setup}

Our experimental evaluation consists of two complementary testing campaigns. For prover testing, we assembled a test suite of over 1000 TPTP problems representing diverse logical domains and difficulty levels. We tested three widely-used provers: E Prover version 3.1, cvc5 version 1.0.8, and Z3 version 4.12.2. Each problem was executed with a timeout of 30 seconds per test. All experiments were conducted on macOS 26.1 running on Apple M3 Max hardware with 64GB RAM.

For integration fuzzing, we curated a collection of 11 high-quality Isabelle theories to serve as mutation seeds. These seed theories were carefully selected to cover diverse Isabelle features including basic arithmetic operations, list manipulations, set operations, logical formulas, inductive proofs, function definitions, option types, pairs, number theory, and relations. Each seed theory contains multiple lemmas with proven proofs, ensuring that our baseline is valid Isabelle code. From these seeds, we generated mutations using our AST-based fuzzer, yielding a total of 204 mutated theories for testing. All mutations were validated using the official Mirabelle tool, providing authoritative verification of our testing results. Additionally, we instrumented Sledgehammer's source code to detect hidden exceptions that might be silently caught by internal try-catch blocks, ensuring comprehensive bug detection beyond surface-level validation.

\subsection{Results}

\subsubsection{Prover Testing Results}

\begin{table}[h]
\centering
\caption{Prover Performance Anomalies Discovered (519 Total)}
\label{tab:prover_bugs}
\begin{tabular}{lrrrr}
\toprule
\textbf{Prover} & \textbf{Timeout} & \textbf{Error} & \textbf{Slowdown} & \textbf{Total} \\
\midrule
E Prover & 186 (53.3\%) & 67 (19.2\%) & 96 (27.5\%) & 349 (67.2\%) \\
cvc5 & 83 (58.0\%) & 41 (28.7\%) & 19 (13.3\%) & 143 (27.6\%) \\
Z3 & 19 (70.4\%) & 7 (25.9\%) & 1 (3.7\%) & 27 (5.2\%) \\
\midrule
\textbf{Total} & \textbf{288 (55.5\%)} & \textbf{115 (22.2\%)} & \textbf{116 (22.3\%)} & \textbf{519 (100\%)} \\
\bottomrule
\end{tabular}
\end{table}

Our differential testing campaign revealed several critical insights into prover reliability. Most strikingly, E Prover accounts for 67.2\% of all discovered anomalies (349 out of 519) despite being one of the most widely deployed and mature theorem provers in the automated reasoning community. Given that all three provers were tested on identical benchmarks, this high count suggests that E Prover's strategy portfolio is more timeout-prone on this problem suite, possibly due to heavier search exploration or less effective strategy scheduling on certain problem types. The anomaly distribution also reveals that E Prover is particularly susceptible to timeout issues, with 186 timeout cases representing 53.3\% of its total anomalies.

Timeout anomalies emerged as the most prevalent issue category overall, accounting for 55.5\% of all discovered anomalies across all provers. This finding has important implications for Sledgehammer users: when automated proof search fails, the most likely cause is a prover timeout rather than an actual error or crash. This suggests that increasing timeout thresholds or implementing more sophisticated timeout management could significantly improve Sledgehammer's success rate.

All three major provers exhibit substantial performance problems, though with different characteristics. E Prover shows a relatively balanced distribution across timeout, error, and slowdown categories, suggesting diverse performance issues. cvc5 demonstrates a higher proportion of timeout anomalies (58.0\%), indicating potential strategy selection issues or incompleteness in certain logical domains. Z3 shows the highest timeout ratio (70.4\%) but the lowest overall anomaly count, suggesting it is generally more reliable but occasionally encounters difficult problems. These anomalies directly impact Sledgehammer's usability: when a user invokes Sledgehammer and it fails to find a proof, the underlying cause is often a prover timeout or error rather than an unprovable goal.

\subsubsection{Integration Fuzzing Results}

\begin{table}[h]
\centering
\caption{Integration Fuzzing Campaign Results (Mirabelle Validation with Hidden Exception Detection)}
\label{tab:fuzzing}
\begin{tabular}{lrrrr}
\toprule
\textbf{Campaign} & \textbf{Mutations} & \textbf{Integration Bugs} & \textbf{Hidden Exceptions} & \textbf{Time (min)} \\
\midrule
Large-Scale Test & 204 & 0 & 0 & 24.3 \\
\midrule
\textbf{Total} & \textbf{204} & \textbf{0} & \textbf{0} & \textbf{24.3} \\
\bottomrule
\end{tabular}
\end{table}

The large-scale fuzzing campaign demonstrated effective mutation generation and comprehensive testing. Testing 204 mutations took 24.3 minutes total, with an average of 7.16 seconds per mutation for Mirabelle validation. This time includes both Sledgehammer execution and Mirabelle's validation logic. The mutation engine successfully employed 9 out of 10 available mutation operators across 11 seed theories, with high operator utilization indicating effective seed selection covering diverse Isabelle features. All 204 mutations were validated using Mirabelle, Isabelle's official testing tool. Mirabelle provides an official, tool-aligned oracle for classifying Sledgehammer outcomes, substantially reducing misclassification risk compared with ad-hoc log parsing. Additionally, we instrumented Sledgehammer's source code to log exceptions caught by internal try-catch blocks, enabling detection of hidden exceptions that might be silently converted to SH\_Unknown status. The instrumentation revealed no hidden exceptions across all 204 tests, confirming that Sledgehammer's defensive exception handling successfully prevented internal errors from propagating while also demonstrating that no internal errors were triggered by our mutations.

\subsubsection{Aggressive Reconstruction Testing Results}

\begin{table}[h]
\centering
\caption{Aggressive Proof Reconstruction Stress Testing (63 Tests)}
\label{tab:aggressive}
\begin{tabular}{lrrr}
\toprule
\textbf{Strategy} & \textbf{Tests} & \textbf{Expected Failures} & \textbf{Reconstruction Bugs} \\
\midrule
Type Breaking & 7 & 4 (type errors) & 0 \\
Encoding Attacks & 5 & 0 & 0 \\
Extreme Values & 6 & 0 & 0 \\
Unicode Injection & 4 & 0 & 0 \\
Boundary Conditions & 5 & 0 & 0 \\
Proof Corruption & 5 & 4 (invalid refs) & 0 \\
Configuration Fuzzing & 5 & 0 & 0 \\
\midrule
Mirabelle Stress Tests & 26 & 1 (lattice ops) & 0 \\
\midrule
\textbf{Total} & \textbf{63} & \textbf{9} & \textbf{0} \\
\bottomrule
\end{tabular}
\end{table}

Our aggressive stress testing campaign (Table~\ref{tab:aggressive}) specifically targeted proof reconstruction vulnerabilities through 63 edge-case tests. The type breaking strategy successfully triggered 4 type errors as expected---Isabelle correctly rejected malformed type combinations (e.g., \texttt{nat + int}) rather than allowing them to propagate to reconstruction. The proof corruption tests caused 4 expected failures when referencing non-existent facts, demonstrating proper error handling. Critically, \textbf{no reconstruction bugs} were discovered: Sledgehammer either correctly processed inputs or properly rejected invalid ones.

The Mirabelle-based stress tests ran actual Sledgehammer proofs on 26 complex formulas spanning nested quantifiers, higher-order functions, type class constraints, multi-prover scenarios, and recursive patterns. Of these, 25 successfully found and reconstructed proofs; one lattice operation test found no proof within the timeout (not a reconstruction failure). The 100\% reconstruction success rate on found proofs further validates Sledgehammer's robustness.


\subsection{Analysis}

\subsubsection{Resilience of the Integration Layer}

The total absence of integration failures across our fuzzing campaign serves as a \textbf{stress test} of Isabelle's architectural maturity. This result, while potentially surprising, is itself a scientifically significant empirical finding that warrants careful interpretation. We consider several complementary explanations.

Unlike the underlying provers, which we found to be susceptible to performance degradation (see Section 5.2.1), the Sledgehammer interface correctly handled all AST-level mutations. The most straightforward explanation is that Isabelle's Sledgehammer integration layer exhibits high stability \textbf{within our tested mutation space}. The Sledgehammer interface has been under active development and use for over a decade, during which time it has been extensively tested both manually and through continuous integration in the Isabelle development process. The interface handles a substantial volume of proof attempts in practice, and many obvious bugs are likely to have been discovered and fixed through this extensive real-world testing. This suggests that the community's engineering efforts on the translation and reconstruction layers have achieved a high degree of correctness, effectively shielding users from internal logic errors even under adversarial input conditions.

Our validation methodology using Mirabelle (Isabelle's official testing tool) combined with source code instrumentation provides confidence in this interpretation. All 204 mutations were validated directly by Mirabelle, ensuring authoritative results. Furthermore, our instrumentation of Sledgehammer's exception handling code enabled detection of hidden exceptions that might be silently caught and converted to SH\_Unknown status. The absence of both surface-level integration bugs and hidden exceptions across all tested mutations, while limited to the tested mutation space, suggests robust integration layer implementation within the scope of our testing.

The aggressive stress testing results (63 tests, 0 reconstruction bugs) further strengthen this conclusion. We specifically designed tests to attack the proof reconstruction pipeline---the most complex part of Sledgehammer that translates external prover outputs back to Isabelle-verifiable proofs. Type breaking attacks, encoding manipulation, extreme values, and proof corruption all failed to trigger reconstruction failures. When Sledgehammer found proofs on complex formulas (nested quantifiers, higher-order functions, type class constraints), reconstruction succeeded 100\% of the time. This demonstrates that Sledgehammer's reconstruction mechanism is remarkably robust against both syntactic edge cases and complex logical structures.

The contrast between 519 prover performance anomalies and the absence of integration failures in our tested mutation space provides insights into the Sledgehammer ecosystem. Within the scope of our testing, reliability issues appear to stem more from the underlying automated theorem provers than from the integration layer. This observation, while limited to our test set, suggests directions for quality assurance efforts. For Sledgehammer users experiencing proof automation failures, prover limitations (timeouts, errors, or inability to find proofs) may be more common causes than integration layer failures, though this requires further investigation beyond our tested mutation space.

\subsubsection{Sledgehammer Capability Boundaries}

Testing with \texttt{Extreme\_Cases.thy} revealed three interesting timeout cases where Sledgehammer exceeded the 30-second threshold on lemmas that native Isabelle tactics proved immediately. These cases expose practical limitations in Sledgehammer's ability to handle certain proof patterns.

One timeout occurred on a lemma about mutually recursive \texttt{even} and \texttt{odd} functions:

\begin{lstlisting}[language=isabelle, caption=Mutual Recursion Pattern (Extreme\_Cases.thy)]
fun even :: "nat => bool" and odd :: "nat => bool" where
  "even 0 = True" |
  "even (Suc n) = odd n" |
  "odd 0 = False" |
  "odd (Suc n) = even n"

lemma even_or_odd: "even n \<or> odd n"
  by (induction n) auto  (* Native proof succeeds *)
  (* sledgehammer times out after 30s *)
\end{lstlisting}

Sledgehammer timed out after 30 seconds, yet the proof was trivial using \texttt{by (induction n) auto}. The difficulty appears to stem from encoding mutual recursion into TPTP format---external provers lack native support for mutually defined functions and must work with less natural encodings that increase search complexity.

Another timeout involved a Fibonacci lemma requiring custom induction:

\begin{lstlisting}[language=isabelle, caption=Custom Induction Pattern (Extreme\_Cases.thy)]
function fib :: "nat => nat" where
  "fib 0 = 0" |
  "fib (Suc 0) = 1" |
  "fib (Suc (Suc n)) = fib (Suc n) + fib n"
  by pat_completeness auto
termination by (relation "measure id") auto

lemma fib_positive: "n > 0 ==> fib n > 0"
  by (induction n rule: fib.induct) auto  (* Native proof succeeds *)
  (* sledgehammer times out *)
\end{lstlisting}

Again, Sledgehammer failed while native tactics succeeded immediately. This suggests that user-defined induction principles, which are common in realistic Isabelle developments, do not map cleanly to the first-order reasoning that external provers employ. Similar issues arose with complex nested set operations involving multiple quantifiers and structural recursion.

These timeouts are not integration bugs in the technical sense---Sledgehammer correctly reports that no proof was found within the time limit and returns control to the user. However, they reveal where Sledgehammer's automation falls short. Users working with mutual recursion, custom induction schemes, or complex nested operations should expect Sledgehammer to struggle and may save time by immediately using native tactics. For tool developers, these patterns suggest specific encoding improvements: better support for recursive definitions in TPTP translation, or hybrid strategies that recognize when problems are better suited to native tactics than external provers.

\subsubsection{Prover Bug Impact}

Although the 519 discovered performance anomalies reside in the external provers rather than Sledgehammer's integration layer, they have direct and significant impact on Sledgehammer's effectiveness as a proof automation tool. Understanding this impact requires recognizing that Sledgehammer's value to users depends entirely on its ability to find proofs, which in turn depends on prover reliability and performance.

The usability impact is most immediately visible through timeout anomalies, which constitute 288 of our 519 discoveries. When a user invokes Sledgehammer on a lemma and waits for automated proof discovery, a prover timeout means Sledgehammer cannot assist, forcing the user to construct the proof manually. Even though the lemma might be provable (as evidenced by other provers succeeding), the timeout in one prover reduces the overall success probability of Sledgehammer's multi-prover strategy. Given that Sledgehammer typically runs multiple provers in parallel and returns the first successful result, timeout anomalies directly reduce the probability of proof discovery and increase the time users spend on manual proof construction.

The reliability impact manifests through the 115 error cases we discovered. When a prover crashes or returns an error, it not only fails to find a proof but may also cause Sledgehammer to report failure even when other provers might have succeeded. Depending on Sledgehammer's error handling strategy, a prover error might trigger early termination of the proof search or cause confusing error messages to be presented to the user. In the worst case, errors could indicate soundness issues in the prover, though we have no evidence of such issues in our testing.

The performance impact accumulates from the 116 slowdown cases, where provers succeed but take dramatically longer than alternatives. While these cases do not represent complete failures, they degrade the user experience by increasing the latency of proof automation. In interactive proof development, where users may invoke Sledgehammer hundreds of times during a session, even seemingly small delays accumulate into substantial productivity losses. Furthermore, in batch processing scenarios such as continuous integration testing of large proof developments, prover slowdowns can make the difference between feasible and infeasible testing times.

Collectively, these findings are valuable to multiple audiences. For Isabelle developers, the anomaly distribution informs prover selection and configuration decisions---for instance, our data suggests that Z3, despite having fewer total anomalies, might be a more reliable default choice for certain problem types. For prover developers, the 519 anomaly reports provide concrete test cases for performance optimization and regression testing. For Sledgehammer users, our findings explain why proof automation sometimes fails and suggest mitigation strategies such as adjusting timeout settings or manually selecting different prover combinations.

\section{Discussion}

\subsection{Contributions to Testing Methodology}

Our work makes several contributions to fuzzing and testing methodology that extend beyond the specific domain of theorem prover testing. Our approach demonstrates the value of leveraging official validation tools directly for bug detection. Rather than developing custom oracles that require training and validation, we use Mirabelle (Isabelle's official testing tool) directly, ensuring results are authoritative by design. This methodology is applicable in domains where official validation tools exist and can be integrated into automated testing workflows.

The techniques we employed for bug detection represent practical insights for testing complex systems. Our approach demonstrates that leveraging official validation tools directly can simplify testing workflows while ensuring result accuracy. In Isabelle's case, Mirabelle provides built-in logic for distinguishing theory-level errors from integration bugs, eliminating the need for custom classification logic. This separation of concerns---where theory errors (syntax, type issues) are correctly identified as distinct from integration failures (Sledgehammer crashes, TPTP errors)---is critical for accurate bug detection in mutation-based testing.

\subsection{Practical Impact}

Our work provides immediate practical value to multiple stakeholder communities. 

\textbf{For Isabelle developers and maintainers}, our testing campaign provides \textbf{fuzzing-based evidence} that the integration layer is stable \textbf{within the tested mutation space}. The fact that all 204 diverse mutations plus 63 aggressive stress tests (267 total tests) were handled without integration failures, reconstruction bugs, or hidden exceptions within our tested mutation space, validated by official Mirabelle testing and confirmed through source code instrumentation, suggests that the interface handles these tested edge cases correctly. Additionally, our identification of problematic prover configurations (specifically, which provers timeout or error on which types of problems) can inform default prover selection strategies in future Isabelle releases. Perhaps most valuably, our framework provides a ready-made regression testing infrastructure that can be deployed whenever Sledgehammer's encoding strategies or integration logic are modified, ensuring that changes do not introduce new bugs.

\textbf{For prover developers}, particularly those maintaining E Prover, cvc5, and Z3, our work delivers 519 concrete anomaly reports documenting specific performance issues. Each report includes the triggering TPTP problem, the observed behavior (timeout, error, or slowdown), and comparison with other provers' behaviors on the same problem. Analysis of these anomalies reveals patterns of problematic problems---for instance, certain quantifier structures or arithmetic reasoning patterns that consistently trigger timeouts in E Prover but not in cvc5. These patterns represent concrete opportunities for optimization and strategy improvement that could enhance prover performance on realistic theorem proving workloads.

\textbf{For the broader research community}, our work contributes both methodology and empirical insights. Our approach demonstrates the value of leveraging official validation tools directly in automated testing workflows, combined with source code instrumentation and aggressive stress testing for comprehensive bug detection. Our AST-based mutation strategies for logical formulas represent a novel application of grammar-aware fuzzing to theorem proving, and could be adapted to other formal methods tools. The empirical finding that Sledgehammer's integration is stable (0 failures across 267 tests including aggressive reconstruction attacks) while underlying provers have numerous performance anomalies (519 cases) challenges assumptions about where reliability problems occur in proof assistant ecosystems.

\subsection{Limitations and Future Work}

Several limitations of our current work suggest promising directions for future research.

\textbf{Mutation Operators}: Our mutation operators, while diverse, focus primarily on syntactic and logical transformations that change the structure or operators in formulas. Future work could explore semantic-preserving mutations that maintain the provability of lemmas while varying their syntactic presentation. Such mutations would test whether Sledgehammer handles semantically equivalent but syntactically different formulations consistently. Type-based mutations that specifically target Isabelle's type system could explore how Sledgehammer handles polymorphism, type classes, and type inference. Domain-specific mutation patterns derived from common proof patterns in the Archive of Formal Proofs could increase the likelihood of triggering domain-specific bugs.

\textbf{Campaign Scale}: The scale of our fuzzing campaign, while sufficient to demonstrate framework capabilities, represents only a fraction of the input space that could be explored. Our 204 mutations provide good coverage of the implemented mutation operators and seed theory diversity across 11 different seed theories, but campaigns with thousands or tens of thousands of mutations could potentially reveal rarer edge cases that only manifest under unusual combinations of features. Larger-scale campaigns could explore more of the input space, though our current scale was constrained by the time required for Mirabelle validation.

\textbf{Code Coverage}: Our current evaluation lacks quantitative code coverage metrics for the Sledgehammer implementation itself. While we achieve good diversity in terms of mutation types and seed theories, we cannot quantify what fraction of Sledgehammer's code paths are actually exercised by our tests. Instrumenting Isabelle to collect coverage data during fuzzing would enable coverage-guided mutation, where mutation strategies are adapted based on which code paths remain unexplored. This feedback-driven approach, successfully employed in tools like AFL for binary fuzzing, could significantly improve the efficiency of bug discovery in Sledgehammer.

\textbf{Framework Specificity}: Our framework is currently specific to Isabelle/HOL, but the core concepts and techniques are generalizable to other proof assistants that integrate external provers. Coq's CoqHammer, Lean's automation tactics, and HOL4's Metis all face similar integration challenges. Adapting our framework to these systems would both validate the generalizability of our approach and potentially discover bugs in these other critical proof assistant tools. Such cross-system evaluation would also enable comparative analysis of different design choices in proof assistant integration layers.

\subsection{Threats to Validity}

Several factors may limit the generalizability of our findings and warrant careful consideration when interpreting results.

\textbf{Software Versions}: External validity is constrained by our specific software versions. All testing was conducted using Isabelle2025, E Prover version 3.1, cvc5 version 1.0.8, and Z3 version 4.12.2. The performance anomalies we identified are specific to these versions, and different versions may exhibit different behaviors. We note that these anomalies represent performance discrepancies observed under our testing conditions; whether they constitute confirmed bugs depends on upstream prover developer evaluation. This is particularly relevant given the active development of all these tools---bug fixes, performance improvements, and new features are regularly added. Our results therefore represent a snapshot of reliability at a specific point in time rather than timeless characteristics of these tools. Replication studies using different versions would be valuable for tracking how reliability evolves over time and whether specific bugs are fixed in newer releases.

\textbf{Seed Theory Corpus}: The selection and size of our seed theory corpus presents another validity consideration. While our 11 seed theories were carefully chosen to cover diverse Isabelle features including arithmetic, lists, sets, logic, induction, functions, options, pairs, numbers, and relations, they represent only a small fraction of Isabelle's extensive feature set. Features such as locales, type classes, quotient types, transfer reasoning, and many advanced proof techniques are not covered by our current seeds. A larger and more diverse seed corpus drawn from the Archive of Formal Proofs or real-world Isabelle projects could improve feature coverage and potentially reveal bugs that only manifest with more complex Isabelle constructs.

\textbf{Timeout Threshold}: Our choice of a 30-second timeout threshold for prover testing, while informed by common practice in the automated reasoning community, is somewhat arbitrary. Different timeout settings could reveal different bug patterns: shorter timeouts would classify more cases as timeouts (potentially inflating bug counts), while longer timeouts would give provers more opportunity to succeed (potentially reducing observed timeout bugs but increasing total testing time). The appropriate timeout depends on the use case---interactive proof development might tolerate only 5-second delays, while batch verification might accept 60-second timeouts. Our results should therefore be interpreted as relative to the 30-second threshold rather than as absolute measures of prover reliability.

\textbf{Encoding Equivalence}: E Prover consumes TPTP natively whereas cvc5 and Z3 consume SMT-LIB encodings derived from the same TPTP problems. Discrepancies may therefore reflect differences introduced by the translation/encoding pipeline rather than prover internals alone. Our findings should be read as toolchain-level performance anomalies relevant to Sledgehammer-style workflows, not necessarily isolated prover implementation bugs. We mitigate this concern by using Isabelle's standard TPTP-to-SMT-LIB conversion pipeline that is production-tested in Sledgehammer, and by focusing on discrepancies where multiple provers succeed (indicating semantic equivalence).

\textbf{Resource Contention}: Running provers concurrently can introduce system-level contention and timing noise. Our timeout and error classifications use concurrent execution (appropriate for detecting these anomalies), but slowdown measurements are performed sequentially with CPU affinity binding and multiple trials to minimize variance. Future work will further isolate resource effects through containerized execution or dedicated hardware per prover.

\textbf{Platform Specificity}: All our experiments were conducted on a single platform: macOS 26.1 running on Apple M3 Max hardware with 64GB RAM. Platform-specific differences in process management, file I/O performance, memory management, and floating-point arithmetic could potentially affect prover behavior. Linux and Windows platforms, which are more commonly used in automated reasoning research and production deployments, might exhibit different anomaly patterns. While we have no specific reason to believe our anomalies are platform-specific, replication on multiple platforms would strengthen confidence in the generalizability of our findings.

\section{Conclusion}

This paper presents a holistic testing framework for the Isabelle Sledgehammer ecosystem, addressing both prover reliability and integration resilience through a dual approach.

This work makes four key empirical and methodological achievements. First, through rigorous differential testing of over 1000 TPTP problems across three major automated theorem provers, we identified 519 performance anomalies (potential performance bugs) encompassing timeouts, errors, and slowdowns. These anomalies represent concrete, reproducible discrepancies that impact real-world users of Sledgehammer. The scale and methodical nature of this bug discovery demonstrates the effectiveness of differential testing for automated reasoning tools.

Second, we developed and deployed a complete AST-based fuzzer specifically designed for Isabelle theories, implementing 10 distinct mutation operators that target different aspects of the Sledgehammer integration. Unlike generic fuzzing tools that operate on bytes or tokens, our fuzzer understands Isabelle's grammar and can perform semantically meaningful transformations while maintaining syntactic validity. This grammar-aware approach represents an advance over naive mutation strategies and could serve as a template for fuzzing other domain-specific languages.

Third, we validated our AST-based fuzzer through systematic testing using Mirabelle, Isabelle's official testing tool, combined with source code instrumentation to detect hidden exceptions. Testing 204 mutations found no integration failures and no hidden exceptions within the tested mutation space. We further validated these findings through aggressive stress testing with 63 edge-case tests specifically targeting proof reconstruction---including type breaking, encoding attacks, extreme values, and boundary conditions---all of which Sledgehammer handled correctly with 0 reconstruction bugs. This comprehensive validation demonstrates both the robustness of Sledgehammer's integration layer and the value of combining official tool validation with targeted stress testing.

Fourth, our mutation-based evaluation establishes a \textbf{lower bound on the reliability} of the Sledgehammer interface on the tested mutation space. By withstanding 204 valid structural mutations without a single failure or hidden exception---validated by Mirabelle and confirmed through source code instrumentation---we provide evidence that, in our evaluation, instability appears confined to the external provers (519 performance anomalies) rather than the integration logic. This finding helps direct future testing efforts toward prover performance rather than interface reliability.

From a methodological perspective, we contribute several generalizable techniques. Our approach demonstrates the value of integrating official validation tools directly into automated testing workflows rather than developing custom oracles. Our AST-based mutation strategies for logical formulas extend grammar-aware fuzzing to the theorem proving domain and could be adapted to other formal methods tools. The fuzzer design provides a template that other researchers can follow when developing mutation-based testing frameworks for systems where official testing tools exist.

The practical impact of our work extends to both users and developers. Within the tested mutation space, we found no evidence of integration failures; in our evaluation, proof automation failures are more plausibly attributable to backend prover limitations (timeouts/errors/slowdowns) than to interface-level crashes. Isabelle developers gain both reassurance about interface stability and concrete prover performance data that can inform prover selection and configuration decisions. Prover developers receive detailed anomaly reports that can guide optimization efforts. The broader formal methods community gains a reusable testing framework and a demonstration that methodical testing can yield significant anomaly discoveries while maintaining scientific rigor.

Future work should pursue several directions. Extended fuzzing campaigns with thousands of mutations could reveal rarer edge cases. Adaptation of our framework to other proof assistants such as Coq, Lean, or HOL4 would test the generalizability of our approach. Coverage-guided mutation, where mutation strategies are informed by code coverage data from Sledgehammer, could improve bug-finding efficiency. Investigation of semantic-preserving mutations that maintain lemma provability while varying syntactic structure could test different aspects of the integration. Finally, longitudinal studies tracking bug discovery across Isabelle and prover versions could provide insights into software evolution and regression patterns in formal methods tools.

This work demonstrates that rigorous testing of proof assistant ecosystems can yield significant anomaly discoveries while maintaining scientific rigor through official tool validation. The 519 prover performance anomalies discovered through differential testing, combined with Sledgehammer's demonstrated stability across 267 diverse tests (including aggressive proof reconstruction attacks), represent concrete contributions to formal verification reliability and provide actionable insights for both prover developers and proof assistant users.

\section*{Acknowledgments}

We thank Dr. Mohammad Ahmad Abdulaziz Ali Mansour and Dr. Karine Even Mendoza for supervision and guidance. This work was supported by the Knowledge Exchange Projects program at King's College London in collaboration with Amazon.

\section*{Data Availability}

Source code, test data, and bug reports are available at the project repository. All experiments are fully reproducible following the provided documentation.

\begin{thebibliography}{99}

\bibitem{afp}
Archive of Formal Proofs. (n.d.). \textit{Archive of Formal Proofs (AFP)}. Retrieved November 29, 2025, from \url{https://www.isa-afp.org/}

\bibitem{afl}
Zalewski, M. (n.d.). \textit{American Fuzzy Lop (AFL) technical details}. Retrieved November 29, 2025, from \url{https://lcamtuf.coredump.cx/afl/technical_details.txt}

\bibitem{barbosa2022cvc5}
Barbosa, H., Barrett, C. W., Brain, M., Kremer, G., Lachnitt, H., Mann, M., Mohamed, A., Mohamed, M., Niemetz, A., N\"otzli, A., Ozdemir, A., Preiner, M., Reynolds, A., Sheng, Y., Tinelli, C., \& Zohar, Y. (2022). cvc5: A versatile and industrial-strength SMT solver. In \textit{TACAS 2022: Tools and Algorithms for the Construction and Analysis of Systems}, Lecture Notes in Computer Science, Vol. 13243, pp. 415--442. Springer. \url{https://doi.org/10.1007/978-3-030-99524-9_24}

\bibitem{brummayer2009}
Brummayer, R., \& Biere, A. (2009). Fuzzing and delta-debugging SMT solvers. In \textit{International Workshop on Satisfiability Modulo Theories} (SMT), pp. 1--5.

\bibitem{klein2009sel4}
Klein, G., Elphinstone, K., Heiser, G., Andronick, J., Cock, D., Derrin, P., \ldots \& Winwood, S. (2009). seL4: Formal verification of an OS kernel. \textit{Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles} (pp. 207--220).

\bibitem{libfuzzer}
LLVM Project. (n.d.). \textit{LibFuzzer}. Retrieved November 29, 2025, from \url{https://llvm.org/docs/LibFuzzer.html}

\bibitem{oddfuzz2023}
Cao, S., He, B., Sun, X., Ouyang, Y., Zhang, C., Wu, X., Su, T., Bo, L., Li, B., Ma, C., Li, J., \& Wei, T. (2023). \textit{ODDFUZZ: Discovering Java deserialization vulnerabilities via structure-aware directed greybox fuzzing}. \href{https://arxiv.org/abs/2304.04233}{arXiv:2304.04233}.

\bibitem{paulson2012sledgehammer}
Paulson, L. C., \& Blanchette, J. C. (2012). Three years of experience with Sledgehammer, a practical link between automatic and interactive theorem provers. \textit{Journal of Automated Reasoning}, 49(3), 389--405.

\bibitem{squirrel2020}
Zhong, R., Chen, Y., Hu, H., Zhang, H., Lee, W., \& Wu, D. (2020). \textit{SQUIRREL: Testing database management systems with language validity and coverage feedback}. In \textit{Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (CCS '20)} (pp. 955--970). ACM. \url{https://doi.org/10.1145/3372297.3417260}

\bibitem{yang2011}
Yang, X., Chen, Y., Eide, E., \& Regehr, J. (2011). Finding and understanding bugs in C compilers. \textit{ACM SIGPLAN Notices}, 46(6), 283--294.

\bibitem{schulz2013}
Schulz, S. (2013). System description: E 1.8. In \textit{LPAR-19: Logic for Programming, Artificial Intelligence, and Reasoning}, pp. 735--743. Springer.

\bibitem{eprover31}
Schulz, S. (2024). \textit{E 3.1 User Manual}. Retrieved November 29, 2025, from \url{https://wwwlehre.dhbw-stuttgart.de/~sschulz/WORK/E_DOWNLOAD/V_3.1/eprover.pdf}

\bibitem{demoura2008}
de Moura, L., \& Bj\o rner, N. (2008). Z3: An efficient SMT solver. In \textit{TACAS 2008: Tools and Algorithms for the Construction and Analysis of Systems}, Lecture Notes in Computer Science, Vol. 4963, pp. 337--340. Springer.

\bibitem{nipkow2002}
Nipkow, T., Paulson, L. C., \& Wenzel, M. (2002). \textit{Isabelle/HOL --- A Proof Assistant for Higher-Order Logic}. Springer.

\bibitem{mirabelle}
Bulwahn, L., \& Krauss, A. (2011). Mirabelle: Automatic testing of automated reasoning tools. In \textit{PAAR 2011: Workshop on Practical Aspects of Automated Reasoning}, pp. 1--5.

\bibitem{tptp}
Sutcliffe, G. (2009). The TPTP Problem Library and Associated Infrastructure: The FOF and CNF Parts, v1.2.1. \textit{Journal of Automated Reasoning}, 43(4), 337--362. \url{https://doi.org/10.1007/s10817-009-9143-8}

\bibitem{tptp_org}
TPTP Problem Library. (n.d.). Retrieved November 29, 2025, from \url{https://www.tptp.org/}

\bibitem{sledgehammer_manual}
Blanchette, J. C., \& Paulson, L. C. (n.d.). \textit{Sledgehammer User Manual}. Retrieved November 29, 2025, from \url{https://isabelle.in.tum.de/dist/doc/sledgehammer.pdf}

\bibitem{sledgehammer_source}
Isabelle Development Team. (2025). \textit{Sledgehammer Implementation} [Source code]. Retrieved November 29, 2025, from \url{https://github.com/seL4/isabelle/blob/master/src/HOL/Tools/Sledgehammer/sledgehammer.ML}

\bibitem{afl_github}
Zalewski, M. (n.d.). \textit{american fuzzy lop - a security-oriented fuzzer} [Source code]. Retrieved November 29, 2025, from \url{https://github.com/google/AFL}

\bibitem{matryoshka}
Barbosa, H., Blanchette, J. C., Fleury, M., \& Fontaine, P. (2023). Reliable reconstruction of fine-grained proofs in a proof assistant. In \textit{Proceedings of the 28th ACM SIGPLAN International Conference on Functional Programming} (ICFP '23), pp. 1--28. \url{https://doi.org/10.1145/3607847}

\end{thebibliography}

\end{document}

