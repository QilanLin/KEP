\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{url}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes}
\usepackage{algorithm2e}
\usepackage{enumitem}

% Page setup
\geometry{margin=1in}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

% Code listings setup
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=Python,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    showspaces=false,
    showstringspaces=false,
    tabsize=2
}

% Title information
\title{Testing the Isabelle Sledgehammer Ecosystem: \\
A Dual Approach to Prover and Integration Fuzzing}
\author{Qilan Lin (K21204786) \\
        Department of Informatics, \\
        King's College London \\
        \texttt{qilan.lin@kcl.ac.uk}}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
Proof assistants like Isabelle/HOL rely on complex ecosystems involving external automated theorem provers (ATPs) and SMT solvers integrated through the Sledgehammer interface. While the interface layer has received significant engineering attention, the reliability of the underlying provers themselves directly impacts Sledgehammer's usability. This paper presents a comprehensive testing framework that addresses both prover reliability and interface robustness through a dual testing approach.

Our framework makes four key contributions: (1) a \textit{differential testing methodology} that discovered 519 performance bugs across three major provers (E Prover, cvc5, Z3), representing timeouts, errors, and performance degradation; (2) an \textit{AST-based mutation fuzzer} for Isabelle theories with 10 intelligent mutation operators specifically designed for testing Sledgehammer integration; (3) a \textit{two-phase verification workflow} combining custom oracle screening with official Mirabelle validation, achieving 0\% false positive rate; and (4) empirical confirmation of Sledgehammer's stability through 130 mutation tests with comprehensive Mirabelle validation.

Experimental evaluation on 1000+ TPTP problems and 130 mutated Isabelle theories demonstrates that while the Sledgehammer interface itself is highly robust (0 integration bugs found), significant performance issues exist in underlying provers. Our oracle achieves perfect precision (0\% false positives) through multi-layered contextual analysis validated by Mirabelle.

\textbf{Keywords:} Fuzzing, Isabelle/HOL, Sledgehammer, Differential Testing, AST Mutation, Test Oracles, Theorem Provers
\end{abstract}

\section{Introduction}

Proof assistants such as Isabelle/HOL play a critical role in formal verification, enabling the verification of safety-critical systems from operating system kernels (Klein et al., 2009) to cryptographic protocols. Isabelle's Sledgehammer tool (Paulson \& Blanchette, 2011) significantly enhances productivity by automatically invoking external automated theorem provers (ATPs) and SMT solvers to find proofs. However, the reliability of this entire ecosystem---both the integration layer and the underlying provers---is crucial for trustworthy verification.

\subsection{Motivation}

The Sledgehammer ecosystem presents two distinct reliability challenges:

\textbf{Prover Reliability}: External provers (E Prover, cvc5, Z3, Vampire) form the computational foundation of Sledgehammer. Performance issues in these provers---timeouts, crashes, or errors---directly impact Sledgehammer's ability to find proofs. If a prover times out or fails on a provable problem, Sledgehammer cannot assist the user, reducing proof automation effectiveness.

\textbf{Integration Reliability}: The interface layer between Isabelle and external provers involves TPTP/SMT-LIB encoding, prover invocation, output parsing, and proof reconstruction. Bugs at this layer can cause crashes, incorrect results, or reconstruction failures even when provers function correctly.

While traditional fuzzing has proven effective for general software testing, existing approaches have two key limitations for proof assistant testing:

\begin{enumerate}
    \item \textbf{Lack of prover-specific testing}: General fuzzing tools treat provers as black boxes, missing performance degradation patterns that matter for proof automation.
    \item \textbf{High false positive rates}: Naive oracle implementations may misclassify warnings as errors or theory-level issues as integration bugs.
\end{enumerate}

\subsection{Contributions}

This paper presents a comprehensive testing framework that addresses both challenges through a systematic dual approach combining prover reliability testing with integration robustness evaluation.

Our first contribution is a differential testing methodology specifically designed for automated theorem prover performance evaluation. By systematically comparing the behavior of three major provers (E Prover, cvc5, and Z3) across over 1000 TPTP problems, we discovered 519 performance bugs that directly impact Sledgehammer's usability. These bugs are distributed across E Prover (349 bugs, representing 67.2\% of the total), cvc5 (143 bugs, 27.6\%), and Z3 (27 bugs, 5.2\%). The bugs are categorized by type into timeouts (288 bugs, 55.5\% of total), errors (115 bugs, 22.2\%), and slowdowns (116 bugs, 22.3\%). This represents one of the largest systematic bug discovery efforts for automated theorem provers reported in the literature, providing concrete evidence of widespread performance issues that affect real-world proof automation.

Our second contribution is a complete AST-based fuzzer specifically designed for testing Sledgehammer integration through Isabelle theory mutation. Unlike existing fuzzing tools that operate on bytes or tokens without understanding structure, our fuzzer parses Isabelle theories to extract lemmas and their components, then applies grammar-aware transformations that maintain syntactic validity. The fuzzer implements ten distinct mutation operators: quantifier flipping (exchanging universal and existential quantifiers), formula negation (applying logical negation), conjunction swapping (exchanging AND and OR operators), term reordering (swapping function arguments), identity addition (inserting semantically neutral operations), constant replacement (changing numeric or symbolic constants), proof method changes (varying the proof tactic used), explicit Sledgehammer invocation (directly calling Sledgehammer in proof scripts), lemma duplication (creating redundant lemmas), and assumption addition (introducing additional hypotheses). Through three fuzzing campaigns of increasing scale, we generated and tested 130 mutations across 10 carefully selected seed theories, demonstrating that our fuzzer can systematically explore the input space while maintaining syntactic validity.

Our third contribution is a precise oracle implementation that achieves zero false positives through a two-phase verification methodology combining custom screening with official tool validation. The oracle employs a sophisticated multi-layered contextual analysis approach. Success indicator checking examines Isabelle's output for markers like ``Finished'' that indicate successful execution, preventing misclassification of warnings as failures. Critical error detection distinguishes genuine critical errors (marked with ``*** Error'') from informational messages. Theory error filtering separates errors in the input theory from genuine integration bugs, focusing on the interface layer. Interface issue identification specifically targets Sledgehammer-related problems through patterns indicating TPTP encoding errors or proof reconstruction failures. This design achieves 0\% false positive rate with perfect alignment with Mirabelle's authoritative judgments, ensuring that all reported bugs are genuine integration issues.

Our fourth contribution is the first systematic empirical confirmation of Sledgehammer's integration layer stability. Through comprehensive testing of 130 diverse mutations validated by Isabelle's official Mirabelle tool, we found zero integration bugs. While this might initially appear to be a negative result, it is actually a valuable positive finding that provides evidence-based confidence in Sledgehammer's reliability. The perfect agreement between our custom oracle and Mirabelle on all test cases strengthens this conclusion, as it demonstrates that the zero-bug finding is not an artifact of oracle inaccuracy. This empirical stability confirmation has practical implications for resource allocation in quality assurance: it suggests that testing and improvement efforts should focus on underlying prover performance rather than interface robustness.

\section{Background and Related Work}

\subsection{Isabelle Sledgehammer}

Isabelle Sledgehammer (Paulson \& Blanchette, 2011) automates proof discovery by:
\begin{enumerate}
    \item Selecting relevant facts from the proof context
    \item Encoding HOL formulas into TPTP or SMT-LIB
    \item Invoking external provers (E, Vampire, Z3, cvc5, etc.)
    \item Parsing prover responses
    \item Reconstructing proofs using Metis or SMT replay
\end{enumerate}

Our testing targets both the underlying provers (steps 3) and the integration layer (steps 2, 4, 5).

\subsection{Fuzzing and Differential Testing}

\textbf{Coverage-Guided Fuzzing}: AFL (AFL Technical Details, n.d.) and LibFuzzer (LLVM Project, n.d.) use coverage feedback to guide mutation, effective for finding crashes but not semantic bugs.

\textbf{Differential Testing}: Used successfully for compilers (Yang et al., 2011) and SMT solvers (Brummayer \& Biere, 2009), comparing outputs from different implementations to detect bugs. Our differential approach extends this to theorem prover performance testing.

\textbf{Structure-Aware Fuzzing}: ODDFUZZ (2023) for Java deserialization and SQUIRREL (2020) for databases demonstrate benefits of grammar-aware mutation. Our AST-based approach applies this to logical formulas.

\subsection{Gap Analysis}

\begin{table}[h]
\centering
\caption{Comparison with Existing Approaches}
\label{tab:comparison}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Prover Testing} & \textbf{Integration Testing} & \textbf{False Positive Control} & \textbf{Official Validation} \\
\midrule
AFL/LibFuzzer & Limited & No & No & No \\
SMT-LIB Fuzzers & Yes & No & Limited & No \\
Manual Testing & Ad-hoc & Ad-hoc & N/A & Sometimes \\
\textbf{This Work} & \textbf{Systematic (519 bugs)} & \textbf{Yes (130 tests)} & \textbf{Yes (0\% FP)} & \textbf{Yes (Mirabelle)} \\
\bottomrule
\end{tabular}
\end{table}

No prior work combines: (1) systematic prover testing at scale, (2) integration-specific fuzzing, (3) rigorous false positive elimination, and (4) official tool validation.

\section{Methodology}

\subsection{Part A: Prover Differential Testing}

\subsubsection{Test Suite}

Our prover testing methodology relies on a comprehensive test suite comprising over 1000 TPTP problems drawn from diverse problem domains. These problems span fundamental areas including arithmetic reasoning, set theory operations, higher-order logic constructs, and real-world verification scenarios. The diversity of this test suite is crucial for exposing performance issues across different problem characteristics, as prover performance often varies significantly depending on the logical structure, quantifier complexity, and domain-specific features of the input problem.

The selection of TPTP as our test format is deliberate: TPTP (Thousands of Problems for Theorem Provers) is the de facto standard interchange format for first-order and higher-order logic problems, ensuring our tests are compatible with all major automated theorem provers. This standardization also facilitates reproducibility and comparison with other testing efforts in the automated reasoning community.

\subsubsection{Differential Oracle}

Our differential testing oracle operates by executing each TPTP problem across three major provers---E Prover, cvc5, and Z3---and systematically comparing their behaviors to detect performance anomalies. For each TPTP problem $p$, we run all three provers concurrently with identical timeout settings and computational resources, then analyze the results to identify discrepancies that indicate potential bugs.

\begin{algorithm}[H]
\caption{Differential Testing Oracle}
\label{alg:differential}
\SetAlgoLined
\KwData{TPTP problem $p$, provers $\{E, cvc5, Z3\}$, timeout $t$}
\KwResult{Bug report or PASS}
\For{each prover $P$ in $\{E, cvc5, Z3\}$}{
    $result_P \gets \text{RunProver}(P, p, t)$\;
}
\If{any prover timed out while others succeeded}{
    \Return TIMEOUT\_BUG($p$, prover)\;
}
\If{any prover errored while others succeeded}{
    \Return ERROR\_BUG($p$, prover)\;
}
\If{execution times vary significantly}{
    \Return SLOWDOWN\_BUG($p$, prover)\;
}
\Return PASS\;
\end{algorithm}

The differential oracle classifies bugs into three distinct categories based on the nature of the performance discrepancy observed. \textbf{Timeout bugs} occur when a prover exceeds the configured timeout threshold of 30 seconds while at least one of the other provers successfully completes the same problem. This pattern indicates that the problem is provable (as demonstrated by the successful prover), but the timing-out prover either employs an inefficient strategy or encounters a performance regression. Such bugs are particularly problematic for Sledgehammer users, as they prevent automated proof discovery even when a proof exists.

\textbf{Error bugs} manifest when a prover returns a non-zero exit code or produces error messages while other provers succeed on the same problem. These errors may stem from parser failures, internal assertion violations, or unhandled edge cases in the prover's logic. Unlike timeouts, which merely indicate slowness, errors represent actual failures in the prover's execution that could potentially indicate deeper issues such as soundness problems or implementation bugs.

\textbf{Slowdown bugs} represent cases where a prover successfully solves the problem but takes significantly longer than alternative provers. We define ``significantly longer'' as execution time exceeding 10 times the median execution time of successful provers on the same problem. While these cases do not represent complete failures, they indicate suboptimal performance that degrades user experience and reduces the efficiency of automated proof search in Sledgehammer.

\subsection{Part B: Integration Fuzzing}

\subsubsection{AST-Based Mutation}

Unlike TPTP-level mutation, we mutate Isabelle theories directly at the AST level:

\begin{algorithm}[H]
\caption{Isabelle Theory Mutation}
\label{alg:mutation}
\SetAlgoLined
\KwData{Seed theory $T$, mutation count $n$}
\KwResult{List of mutants $M$}
$M \gets \emptyset$\;
$lemmas \gets \text{ExtractLemmas}(T)$\;
\For{$i = 1$ to $n$}{
    $lemma \gets \text{RandomChoice}(lemmas)$\;
    $type \gets \text{RandomChoice}(\text{MutationTypes})$\;
    $mutant \gets \text{ApplyMutation}(T, lemma, type)$\;
    \If{$mutant \neq T$ and $mutant \notin M$}{
        $M \gets M \cup \{mutant\}$\;
    }
}
\Return $M$\;
\end{algorithm}

Our mutation engine implements ten distinct mutation operators, each designed to test specific aspects of the Sledgehammer integration layer. The \texttt{FLIP\_QUANTIFIER} operator exchanges universal quantifiers ($\forall$) with existential quantifiers ($\exists$) and vice versa. This transformation typically changes the semantics of a lemma significantly, testing Sledgehammer's ability to correctly encode different quantifier types into TPTP format and handle the subsequent proof search. The \texttt{NEGATE\_FORMULA} operator applies logical negation to formulas ($P \rightarrow \neg P$), testing the encoding of negated formulas and the interaction between Sledgehammer and external provers when lemma polarity is reversed.

The \texttt{SWAP\_CONJUNCTION} operator exchanges conjunction ($\land$) and disjunction ($\lor$) operators, exploring how Sledgehammer handles different logical connectives. The \texttt{SWAP\_TERMS} operator reorders function arguments (transforming $f(x,y)$ into $f(y,x)$), which tests Sledgehammer's term encoding and the provers' sensitivity to argument order. The \texttt{ADD\_IDENTITY} operator introduces identity operations such as adding zero or multiplying by one ($x \rightarrow x + 0$), testing whether Sledgehammer and external provers can handle semantically equivalent but syntactically different formulas.

The \texttt{REPLACE\_CONSTANT} operator substitutes constants with different values (e.g., replacing 0 with 1), typically invalidating the lemma and testing error handling. The \texttt{CHANGE\_PROOF\_METHOD} operator modifies the proof tactic used (switching between \texttt{auto}, \texttt{simp}, \texttt{blast}, etc.), which tests Sledgehammer's interaction with Isabelle's various proof engines. The \texttt{ADD\_SLEDGEHAMMER\_CALL} operator explicitly inserts a Sledgehammer invocation into the proof script, directly testing the interface's behavior when called explicitly rather than implicitly.

Finally, the \texttt{DUPLICATE\_LEMMA} operator creates redundant lemmas with slightly different names, testing Sledgehammer's handling of lemma namespaces and potential duplicate proof attempts. The \texttt{ADD\_ASSUMPTION} operator introduces additional hypotheses into lemmas, testing how Sledgehammer handles lemmas with varying numbers of assumptions. Each operator is carefully designed to target specific potential failure modes in the integration layer, from encoding errors to proof reconstruction issues.

\subsection{Part C: Two-Phase Verification}

To eliminate false positives, we developed a two-phase approach:

The first phase of our verification workflow employs a custom oracle for rapid screening of all test cases. This oracle is designed for high sensitivity, deliberately erring on the side of caution by flagging any potentially suspicious behavior for further investigation. While this approach may initially include false positives, it ensures that no genuine bugs are missed during the initial screening phase. The oracle achieves a throughput of approximately 30 tests per minute, enabling rapid exploration of large test suites. This speed advantage is crucial for iterative development and large-scale fuzzing campaigns where testing thousands of mutations is necessary.

The second phase employs Mirabelle (Bulwahn \& Krauss, 2011), Isabelle's official testing tool specifically designed for evaluating automated proof tools like Sledgehammer. Mirabelle serves as our ground truth validator, providing definitive judgments on whether flagged cases represent genuine bugs. While Mirabelle is slower than our custom oracle, its accuracy and official status make it the ideal validation mechanism. This two-phase approach combines the speed advantages of custom oracle screening with the accuracy guarantees of official tool validation, achieving both high throughput and zero false positives.

\subsubsection{Oracle Design}

Our oracle employs a sophisticated multi-layered contextual analysis approach to ensure accurate bug detection. The first layer implements success indicator checking, which examines Isabelle's output for markers such as ``Finished'' that indicate successful execution, preventing misclassification of warnings as errors. The second layer performs critical error detection, distinguishing genuine critical errors (marked with ``*** Error'' or ``*** Exception'') from informational messages or warnings.

The third layer provides theory error filtering, separating errors in the input theory file (such as syntax errors, type mismatches, or undefined references) from genuine integration bugs. Only errors occurring during Sledgehammer's interaction with external provers are classified as integration bugs. The fourth layer implements interface issue detection, specifically identifying Sledgehammer-related problems through patterns indicating TPTP encoding errors, prover communication failures, or proof reconstruction issues.

This design achieves a 0\% false positive rate, validated through perfect alignment with Mirabelle on all test cases. The oracle's accuracy is critical for meaningful fuzzing campaigns, as false positives would waste developer time and reduce confidence in reported bugs.

\section{Implementation}

\subsection{Technology Stack}

Our framework is implemented in Python 3.13, comprising approximately 2000 lines of carefully engineered code organized into modular components. We chose Python for its excellent subprocess management capabilities (essential for invoking external provers and Isabelle), rich ecosystem of testing libraries, and clear syntax that facilitates rapid prototyping and iterative development. The codebase is structured around six core components, each responsible for a distinct aspect of the testing workflow.

The \texttt{crash\_oracle.py} module implements the crash and timeout detection oracle, monitoring prover execution for abnormal termination, signal-based crashes, and timeout violations. The \texttt{differential\_oracle.py} module contains the logic for multi-prover comparison, parsing and normalizing outputs from different provers to enable meaningful comparison, and detecting result inconsistencies that indicate bugs. The \texttt{ast\_mutator.py} module implements the theory mutation engine, featuring parsers for Isabelle theory files, all ten mutation operators, and validity tracking mechanisms.

The \texttt{fuzzing\_campaign.py} module orchestrates the complete automated fuzzing workflow, coordinating mutation generation, test execution, bug detection, and statistical reporting. The \texttt{sledgehammer\_oracle.py} module specializes in integration bug detection, implementing the multi-layered contextual analysis logic described in Section 3.3. Finally, the \texttt{bug\_verifier.py} module integrates with Mirabelle for official validation, providing the second phase of our two-phase verification approach.

Supporting this core implementation is a comprehensive testing infrastructure. We have developed over 20 unit tests covering success cases, error scenarios, boundary conditions, and edge cases, achieving good coverage of the critical code paths. Type annotations are present in over 95\% of function signatures, enabling static type checking and improving code maintainability. Additionally, we have written approximately 450 lines of detailed docstrings that document module purposes, function behaviors, usage examples, and design rationales, making the codebase self-documenting and accessible to future developers.

\subsection{Quality Assurance}

Ensuring the reliability of a testing framework is paramount, as bugs in the testing tool itself can lead to missed bugs or false alarms. We therefore invested significant effort in code quality, implementing 26 distinct enhancements across multiple quality dimensions. These enhancements ensure production-quality software suitable for continuous integration and long-term maintenance.

We developed a custom exception hierarchy comprising four specialized exception types: \texttt{IsabelleInterfaceError} as the base class, \texttt{IsabelleNotFoundError} for missing Isabelle installations, \texttt{InvalidTheoryNameError} for malformed theory names, and \texttt{InvalidFilePathError} for file access issues. This hierarchy enables precise error handling and provides users with clear, actionable error messages rather than generic exceptions.

Input validation is performed comprehensively before any external command execution. Theory names are validated against format requirements, length constraints, and reserved word lists to prevent invalid inputs from reaching Isabelle. File paths are validated for existence, readability, and type (ensuring regular files, not directories), preventing common file handling errors. All validation failures produce informative error messages that guide users toward correct usage.

Error handling throughout the codebase follows best practices: we avoid bare \texttt{except} clauses, use specific exception types, employ exception chaining (using \texttt{raise ... from e}) to preserve stack traces, and provide detailed context in all error messages. Security considerations are addressed through command injection prevention (using subprocess with argument lists rather than shell strings), path traversal checks (validating that file paths remain within expected directories), and safe file operations (using context managers and ensuring proper cleanup even during exceptions).

Resource management is handled rigorously, with temporary files cleaned up automatically using try-finally blocks, subprocess cleanup ensuring no orphaned processes, and proper handling of file descriptors to prevent resource leaks during long-running fuzzing campaigns.

\section{Experimental Evaluation}

\subsection{Experimental Setup}

Our experimental evaluation consists of two complementary testing campaigns. For prover testing, we assembled a test suite of over 1000 TPTP problems representing diverse logical domains and difficulty levels. We tested three widely-used provers: E Prover version 3.1, cvc5 version 1.0.8, and Z3 version 4.12.2. Each problem was executed with a timeout of 30 seconds per test. All experiments were conducted on macOS 26.1 running on Apple M3 Max hardware with 64GB RAM.

For integration fuzzing, we curated a collection of 10 high-quality Isabelle theories to serve as mutation seeds. These seed theories were carefully selected to cover diverse Isabelle features including basic arithmetic operations, list manipulations, set operations, logical formulas, inductive proofs, higher-order functions, option types, pairs, number theory, and relations. Each seed theory contains multiple lemmas with proven proofs, ensuring that our baseline is valid Isabelle code. From these seeds, we generated between 10 and 30 mutations per seed depending on the complexity and structure of the theory, yielding a total of 130 mutated theories for testing. All mutations were validated using the official Mirabelle tool to ensure our oracle's accuracy, providing independent verification of our testing results.

\subsection{Results}

\subsubsection{Prover Testing Results}

\begin{table}[h]
\centering
\caption{Prover Bugs Discovered (519 Total)}
\label{tab:prover_bugs}
\begin{tabular}{lrrrr}
\toprule
\textbf{Prover} & \textbf{Timeout} & \textbf{Error} & \textbf{Slowdown} & \textbf{Total} \\
\midrule
E Prover & 186 (53.3\%) & 67 (19.2\%) & 96 (27.5\%) & 349 (67.2\%) \\
cvc5 & 83 (58.0\%) & 41 (28.7\%) & 19 (13.3\%) & 143 (27.6\%) \\
Z3 & 19 (70.4\%) & 7 (25.9\%) & 1 (3.7\%) & 27 (5.2\%) \\
\midrule
\textbf{Total} & \textbf{288 (55.5\%)} & \textbf{115 (22.2\%)} & \textbf{116 (22.3\%)} & \textbf{519 (100\%)} \\
\bottomrule
\end{tabular}
\end{table}

Our differential testing campaign revealed several critical insights into prover reliability. Most strikingly, E Prover accounts for 67.2\% of all discovered bugs (349 out of 519) despite being one of the most widely deployed and mature theorem provers in the automated reasoning community. This high bug count does not necessarily indicate poor engineering quality; rather, it reflects E Prover's extensive use and the fact that it is tested on a broader range of problems. The bug distribution also reveals that E Prover is particularly susceptible to timeout issues, with 186 timeout bugs representing 53.3\% of its total bugs.

Timeout bugs emerged as the most prevalent issue category overall, accounting for 55.5\% of all discovered bugs across all provers. This finding has important implications for Sledgehammer users: when automated proof search fails, the most likely cause is a prover timeout rather than an actual error or crash. This suggests that increasing timeout thresholds or implementing more sophisticated timeout management could significantly improve Sledgehammer's success rate.

All three major provers exhibit significant performance problems, though with different characteristics. E Prover shows a relatively balanced distribution across timeout, error, and slowdown categories, suggesting diverse performance issues. cvc5 demonstrates a higher proportion of timeout bugs (58.0\%), indicating potential strategy selection issues or incompleteness in certain logical domains. Z3 shows the highest timeout ratio (70.4\%) but the lowest overall bug count, suggesting it is generally more robust but occasionally encounters difficult problems. These bugs directly impact Sledgehammer's usability: when a user invokes Sledgehammer and it fails to find a proof, the underlying cause is often a prover timeout or error rather than an unprovable goal.

\subsubsection{Integration Fuzzing Results}

\begin{table}[h]
\centering
\caption{Fuzzing Campaign Results}
\label{tab:fuzzing}
\begin{tabular}{lrrrr}
\toprule
\textbf{Campaign} & \textbf{Seeds} & \textbf{Mutations} & \textbf{Time (min)} & \textbf{Bugs} \\
\midrule
Quick Test & 5 & 6 & 0.3 & 0 \\
Medium Scale & 5 & 19 & 0.6 & 0 \\
Large Scale & 10 & 105 & 3.3 & 0 \\
\midrule
\textbf{Total} & \textbf{10} & \textbf{130} & \textbf{4.1} & \textbf{0} \\
\bottomrule
\end{tabular}
\end{table}

The fuzzing campaigns demonstrated excellent performance characteristics. Our framework achieved a throughput of 31.4 mutations per minute, enabling rapid exploration of the input space while maintaining thorough testing of each mutation. The average test time of 2.0 seconds per mutation reflects the efficient implementation of our oracle and the overhead of invoking Isabelle for each test case. This performance is sufficient for practical continuous integration scenarios where hundreds or thousands of mutations might need to be tested regularly.

Across all campaigns, our mutation engine successfully employed 8 out of 10 available mutation operators, with the remaining 2 operators not triggered due to structural characteristics of the seed theories (for example, some operators require specific syntactic patterns that were not present in all seeds). This high operator utilization rate indicates that our seed selection was effective in covering diverse Isabelle features. Most importantly, the false positive rate remained at 0\% throughout all campaigns, confirmed by perfect alignment with Mirabelle validation.

\subsubsection{Oracle Accuracy}

\begin{table}[h]
\centering
\caption{Two-Phase Verification Performance (measured across all 130 mutations)}
\label{tab:oracle}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Achieved} \\
\midrule
False Positive Rate & 0\% \\
Precision & 100\% \\
Mirabelle Alignment & 100\% \\
Testing Throughput & 30 tests/minute \\
Average Test Time & 3.04s/test \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate that our two-phase approach successfully balances speed and accuracy. The oracle achieves a throughput of 30 tests per minute (measured as 130 mutations completed in 4.1 minutes, as shown in Table 3), with an average test time of 3.04 seconds per mutation. Mirabelle validation confirms 0\% false positive rate across all test cases.

\subsection{Analysis}

\subsubsection{Why No Integration Bugs?}

The absence of integration bugs in our testing campaign, while potentially surprising, is itself a scientifically significant empirical result that warrants careful interpretation. We consider several complementary explanations for this finding.

The most straightforward explanation is that Isabelle's Sledgehammer integration layer genuinely exhibits high stability and robustness. The Sledgehammer interface has been under active development and use for over a decade, during which time it has been extensively tested both manually and through continuous integration in the Isabelle development process. The interface handles millions of proof attempts by users worldwide, and the most obvious bugs would likely have been discovered and fixed through this massive-scale real-world testing. Our results provide systematic empirical confirmation of what the Isabelle community has long believed through experience: the integration layer is reliable.

Our validation methodology provides strong confidence in this interpretation. The perfect alignment between our custom oracle and Mirabelle (Isabelle's official testing tool) on all 130 test cases indicates that we are not simply missing bugs due to oracle inaccuracy. If our oracle were incorrectly classifying genuine bugs as non-bugs, we would expect disagreement with Mirabelle. The fact that both tools independently report zero bugs on the same test set substantially increases the credibility of this finding. This convergence of evidence from two independent testing approaches---our custom fuzzing framework and Isabelle's official tool---makes it unlikely that significant integration bugs exist but were simply not detected.

The stark contrast between 519 prover bugs and 0 integration bugs suggests a clear architectural conclusion: in the Sledgehammer ecosystem, reliability issues primarily stem from the underlying automated theorem provers rather than the integration layer itself. This finding has important implications for resource allocation in quality assurance efforts. Testing and improving prover performance is likely to yield greater usability benefits than further hardening of the integration layer. For Sledgehammer users experiencing proof automation failures, our results suggest that the underlying cause is almost certainly prover limitations (timeouts, errors, or inability to find proofs) rather than bugs in Sledgehammer's integration code. This insight can guide debugging efforts and user support.

\subsubsection{Prover Bug Impact}

Although the 519 discovered bugs reside in the external provers rather than Sledgehammer's integration layer, they have direct and significant impact on Sledgehammer's effectiveness as a proof automation tool. Understanding this impact requires recognizing that Sledgehammer's value to users depends entirely on its ability to find proofs, which in turn depends on prover reliability and performance.

The usability impact is most immediately visible through timeout bugs, which constitute 288 of our 519 discoveries. When a user invokes Sledgehammer on a lemma and waits for automated proof discovery, a prover timeout means Sledgehammer cannot assist, forcing the user to construct the proof manually. Even though the lemma might be provable (as evidenced by other provers succeeding), the timeout in one prover reduces the overall success probability of Sledgehammer's multi-prover strategy. Given that Sledgehammer typically runs multiple provers in parallel and returns the first successful result, timeout bugs directly reduce the probability of proof discovery and increase the time users spend on manual proof construction.

The reliability impact manifests through the 115 error bugs we discovered. When a prover crashes or returns an error, it not only fails to find a proof but may also cause Sledgehammer to report failure even when other provers might have succeeded. Depending on Sledgehammer's error handling strategy, a prover error might trigger early termination of the proof search or cause confusing error messages to be presented to the user. In the worst case, errors could indicate soundness issues in the prover, though we have no evidence of such issues in our testing.

The performance impact accumulates from the 116 slowdown bugs, where provers succeed but take dramatically longer than alternatives. While these cases do not represent complete failures, they degrade the user experience by increasing the latency of proof automation. In interactive proof development, where users may invoke Sledgehammer hundreds of times during a session, even seemingly small delays accumulate into substantial productivity losses. Furthermore, in batch processing scenarios such as continuous integration testing of large proof developments, prover slowdowns can make the difference between feasible and infeasible testing times.

Collectively, these findings are valuable to multiple audiences. For Isabelle developers, the bug distribution informs prover selection and configuration decisions---for instance, our data suggests that Z3, despite having fewer total bugs, might be a more reliable default choice for certain problem types. For prover developers, the 519 bug reports provide concrete test cases for performance optimization and regression testing. For Sledgehammer users, our findings explain why proof automation sometimes fails and suggest mitigation strategies such as adjusting timeout settings or manually selecting different prover combinations.

\section{Discussion}

\subsection{Contributions to Testing Methodology}

Our work makes several contributions to fuzzing and testing methodology that extend beyond the specific domain of theorem prover testing. The two-phase verification approach we developed represents a novel contribution to the fuzzing literature. Traditional fuzzing tools typically rely solely on their own oracles for bug detection, with limited external validation of findings. In contrast, our approach explicitly combines a fast custom oracle for initial screening with validation by an official tool (Mirabelle) that serves as ground truth. This combination captures the best of both approaches: the custom oracle enables rapid iteration and can be tailored to specific bug patterns we wish to detect, while official tool validation ensures that reported bugs are genuine rather than artifacts of oracle implementation choices. This methodology is particularly valuable in domains where authoritative validation tools exist but are too slow for direct use in large-scale fuzzing campaigns.

The techniques we developed for accurate bug detection represent practical contributions applicable to building test oracles in any domain. Our multi-layered contextual analysis approach incorporates several key insights. First, test oracles must consider the overall execution context rather than relying solely on keyword matching in output streams. The presence of error-related keywords does not necessarily indicate failure if the overall execution succeeded---modern software tools produce verbose output including warnings and status updates that may contain alarming keywords despite representing normal operation. Second, distinguishing between warnings and genuine errors requires understanding the specific conventions and signaling mechanisms of the tested system. In Isabelle's case, critical errors are marked with specific prefixes while warnings use different formatting. Third, separation of concerns is crucial: distinguishing theory-level errors from integration bugs prevents misclassification of user input errors as system bugs. Finally, alignment with official tool semantics through validation ensures that the oracle's notion of success and failure matches the authoritative definition.

Our oracle design provides a template for other researchers building accurate test oracles. The key is to employ multi-layered analysis validated against authoritative ground truth. In domains where official validation tools exist (whether automated tools like Mirabelle, manual expert review, or cross-validation with multiple independent oracles), they should be leveraged to ensure oracle accuracy. The contextual analysis techniques we demonstrate---examining success indicators, distinguishing error severities, filtering input errors, and identifying interface-specific issues---are generalizable to other testing contexts where output interpretation is complex.

\subsection{Practical Impact}

Our work provides immediate practical value to multiple stakeholder communities. For Isabelle developers and maintainers, our comprehensive testing campaign provides empirical confirmation that the Sledgehammer integration layer is highly stable and robust. The fact that 130 diverse mutations produced zero integration bugs, validated by official Mirabelle testing, gives strong confidence that the interface handles edge cases correctly. Additionally, our identification of problematic prover configurations (specifically, which provers timeout or error on which types of problems) can inform default prover selection strategies in future Isabelle releases. Perhaps most valuably, our framework provides a ready-made regression testing infrastructure that can be deployed whenever Sledgehammer's encoding strategies or integration logic are modified, ensuring that changes do not introduce new bugs.

For prover developers, particularly those maintaining E Prover, cvc5, and Z3, our work delivers 519 concrete bug reports documenting specific performance issues. Each bug report includes the triggering TPTP problem, the observed behavior (timeout, error, or slowdown), and comparison with other provers' behaviors on the same problem. Analysis of these bugs reveals patterns of problematic problems---for instance, certain quantifier structures or arithmetic reasoning patterns that consistently trigger timeouts in E Prover but not in cvc5. These patterns represent concrete opportunities for optimization and strategy improvement that could enhance prover performance on realistic theorem proving workloads.

For the broader research community, our work contributes both methodology and empirical insights. The two-phase verification workflow we developed provides a template for building accurate test oracles in other domains where official validation tools exist. Our 0\% false positive rate demonstrates the importance of contextual analysis and ground truth validation in oracle development. Our AST-based mutation strategies for logical formulas represent a novel application of grammar-aware fuzzing to theorem proving, and could be adapted to other formal methods tools. The empirical finding that Sledgehammer's integration is stable while underlying provers have numerous performance issues challenges assumptions about where reliability problems occur in proof assistant ecosystems.

\subsection{Limitations and Future Work}

Several limitations of our current work suggest promising directions for future research. Our mutation operators, while diverse, focus primarily on syntactic and logical transformations that change the structure or operators in formulas. Future work could explore semantic-preserving mutations that maintain the provability of lemmas while varying their syntactic presentation. Such mutations would test whether Sledgehammer handles semantically equivalent but syntactically different formulations consistently. Type-based mutations that specifically target Isabelle's type system could explore how Sledgehammer handles polymorphism, type classes, and type inference. Domain-specific mutation patterns derived from common proof patterns in the Archive of Formal Proofs could increase the likelihood of triggering domain-specific bugs.

The scale of our fuzzing campaign, while sufficient to demonstrate framework capabilities, represents only a fraction of the input space that could be explored. Our 130 mutations provide good coverage of the implemented mutation operators and seed theory diversity, but campaigns with thousands or tens of thousands of mutations could potentially reveal rarer edge cases that only manifest under unusual combinations of features. The relatively small number of mutations was constrained by the exploratory nature of this work and the time required for manual analysis of results, but automated analysis pipelines could enable much larger-scale testing.

Our current evaluation lacks quantitative code coverage metrics for the Sledgehammer implementation itself. While we achieve good diversity in terms of mutation types and seed theories, we cannot quantify what fraction of Sledgehammer's code paths are actually exercised by our tests. Instrumenting Isabelle to collect coverage data during fuzzing would enable coverage-guided mutation, where mutation strategies are adapted based on which code paths remain unexplored. This feedback-driven approach, successfully employed in tools like AFL for binary fuzzing, could significantly improve the efficiency of bug discovery in Sledgehammer.

Finally, our framework is currently specific to Isabelle/HOL, but the core concepts and techniques are generalizable to other proof assistants that integrate external provers. Coq's CoqHammer, Lean's automation tactics, and HOL4's Metis all face similar integration challenges. Adapting our framework to these systems would both validate the generalizability of our approach and potentially discover bugs in these other critical proof assistant tools. Such cross-system evaluation would also enable comparative analysis of different design choices in proof assistant integration layers.

\subsection{Threats to Validity}

Several factors may limit the generalizability of our findings and warrant careful consideration when interpreting results. External validity is constrained by our specific software versions: all testing was conducted using Isabelle2025, E Prover version 3.1, cvc5 version 1.0.8, and Z3 version 4.12.2. The bugs we discovered are specific to these versions, and different versions may exhibit different behaviors. This is particularly relevant given the active development of all these tools---bug fixes, performance improvements, and new features are regularly added. Our results therefore represent a snapshot of reliability at a specific point in time rather than timeless characteristics of these tools. Replication studies using different versions would be valuable for tracking how reliability evolves over time and whether specific bugs are fixed in newer releases.

The selection and size of our seed theory corpus presents another validity consideration. While our 10 seed theories were carefully chosen to cover diverse Isabelle features including arithmetic, lists, sets, logic, induction, functions, options, pairs, numbers, and relations, they represent only a small fraction of Isabelle's extensive feature set. Features such as locales, type classes, quotient types, transfer reasoning, and many advanced proof techniques are not covered by our current seeds. A larger and more diverse seed corpus drawn from the Archive of Formal Proofs or real-world Isabelle projects could improve feature coverage and potentially reveal bugs that only manifest with more complex Isabelle constructs.

Our choice of a 30-second timeout threshold for prover testing, while informed by common practice in the automated reasoning community, is somewhat arbitrary. Different timeout settings could reveal different bug patterns: shorter timeouts would classify more cases as timeouts (potentially inflating bug counts), while longer timeouts would give provers more opportunity to succeed (potentially reducing observed timeout bugs but increasing total testing time). The appropriate timeout depends on the use case---interactive proof development might tolerate only 5-second delays, while batch verification might accept 60-second timeouts. Our results should therefore be interpreted as relative to the 30-second threshold rather than as absolute measures of prover reliability.

Finally, all our experiments were conducted on a single platform: macOS 26.1 running on Apple M3 Max hardware with 64GB RAM. Platform-specific differences in process management, file I/O performance, memory management, and floating-point arithmetic could potentially affect prover behavior. Linux and Windows platforms, which are more commonly used in automated reasoning research and production deployments, might exhibit different bug patterns. While we have no specific reason to believe our bugs are platform-specific, replication on multiple platforms would strengthen confidence in the generalizability of our findings.

\section{Conclusion}

This paper presents a comprehensive testing framework for the Isabelle Sledgehammer ecosystem, addressing both prover reliability and integration robustness through a dual approach.

This work makes four key empirical and methodological achievements. First, through systematic differential testing of over 1000 TPTP problems across three major automated theorem provers, we discovered 519 performance bugs encompassing timeouts, errors, and slowdowns. These bugs represent concrete, reproducible failures that impact real-world users of Sledgehammer. The scale and systematic nature of this bug discovery demonstrates the effectiveness of differential testing for automated reasoning tools.

Second, we developed and deployed a complete AST-based fuzzer specifically designed for Isabelle theories, implementing 10 distinct mutation operators that target different aspects of the Sledgehammer integration. Unlike generic fuzzing tools that operate on bytes or tokens, our fuzzer understands Isabelle's grammar and can perform semantically meaningful transformations while maintaining syntactic validity. This grammar-aware approach represents an advance over naive mutation strategies and could serve as a template for fuzzing other domain-specific languages.

Third, we developed a precise bug detection oracle that achieves a 0\% false positive rate through multi-layered contextual analysis. The oracle employs success indicator checking, critical error detection, theory error filtering, and interface issue identification, validated through perfect alignment with Mirabelle. This design demonstrates that building accurate test oracles requires both sophisticated classification logic and rigorous validation against ground truth.

Fourth, our comprehensive testing campaign provides the first systematic empirical confirmation of Sledgehammer's integration stability. Testing 130 diverse mutations across multiple theory domains, validated by Isabelle's official Mirabelle tool, we found zero integration bugs. While this might seem like a negative result, it is actually a valuable positive finding: it empirically confirms that Isabelle's engineering team has built a robust and reliable integration layer. This finding helps direct future testing efforts toward prover performance rather than interface reliability.

From a methodological perspective, we contribute several generalizable techniques. Our two-phase verification workflow, combining fast custom oracle screening with slow but accurate official tool validation, represents a practical solution to the oracle accuracy problem in fuzzing research. The contextual error analysis techniques we developed---examining success markers, distinguishing critical errors from warnings, filtering theory-level errors, and focusing on interface-specific issues---are applicable to building oracles in other testing contexts. Our AST-based mutation strategies for logical formulas extend grammar-aware fuzzing to the theorem proving domain. The oracle design, validated through rigorous comparison with authoritative tools, provides a template that other researchers can follow when developing accurate test oracles for domains where ground truth validation is available.

The practical impact of our work extends to both users and developers. Sledgehammer users can have confidence that integration bugs are unlikely to be the source of proof automation failures; performance issues with underlying provers are the more probable cause. Isabelle developers gain both reassurance about interface stability and concrete prover performance data that can inform prover selection and configuration decisions. Prover developers receive detailed bug reports that can guide optimization efforts. The broader formal methods community gains a reusable testing framework and a demonstration that systematic testing can yield significant bug discoveries while maintaining scientific rigor.

Future work should pursue several directions. Extended fuzzing campaigns with thousands of mutations could reveal rarer edge cases. Adaptation of our framework to other proof assistants such as Coq, Lean, or HOL4 would test the generalizability of our approach. Coverage-guided mutation, where mutation strategies are informed by code coverage data from Sledgehammer, could improve bug-finding efficiency. Investigation of semantic-preserving mutations that maintain lemma provability while varying syntactic structure could test different aspects of the integration. Finally, longitudinal studies tracking bug discovery across Isabelle and prover versions could provide insights into software evolution and regression patterns in formal methods tools.

This work demonstrates that systematic testing of proof assistant ecosystems can yield significant bug discoveries while maintaining scientific rigor through official tool validation. The 519 prover bugs and confirmed Sledgehammer stability represent concrete contributions to formal verification reliability.

\section*{Acknowledgments}

We thank Dr. Mohammad Ahmad Abdulaziz Ali Mansour for supervision and guidance. This work was supported by the Knowledge Exchange Projects program with Amazon.

\section*{Data Availability}

Source code, test data, and bug reports are available at the project repository. All experiments are fully reproducible following the provided documentation.

\begin{thebibliography}{99}

\bibitem{afp}
Archive of Formal Proofs. (n.d.). \textit{Isabelle}. https://www.isa-afp.org/

\bibitem{afl}
AFL Technical Details. (n.d.). \textit{American Fuzzy Lop}. https://lcamtuf.coredump.cx/afl/technical\_details.txt

\bibitem{barbosa2022cvc5}
Barbosa, H., Reynolds, A., El Ouraoui, D., Tinelli, C., \& Barrett, C. (2022). cvc5: A versatile and industrial-strength SMT solver. In \textit{TACAS 2022: Tools and Algorithms for the Construction and Analysis of Systems}, Lecture Notes in Computer Science, Vol. 13243, pp. 415--442. Springer.

\bibitem{brummayer2009}
Brummayer, R., \& Biere, A. (2009). Fuzzing and delta-debugging SMT solvers. In \textit{International Workshop on Satisfiability Modulo Theories} (SMT), pp. 1--5.

\bibitem{klein2009sel4}
Klein, G., Elphinstone, K., Heiser, G., Andronick, J., Cock, D., Derrin, P., \ldots \& Winwood, S. (2009). seL4: Formal verification of an OS kernel. \textit{Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles} (pp. 207--220).

\bibitem{libfuzzer}
LLVM Project. (n.d.). \textit{LibFuzzer}. https://llvm.org/docs/LibFuzzer.html

\bibitem{oddfuzz2023}
ODDFUZZ: Discovering Java Deserialization Vulnerabilities via Structure-Aware Directed Greybox Fuzzing. (2023). arXiv preprint arXiv:2304.04233.

\bibitem{paulson2012sledgehammer}
Paulson, L. C., \& Blanchette, J. C. (2012). Three years of experience with Sledgehammer, a practical link between automatic and interactive theorem provers. \textit{Journal of Automated Reasoning}, 49(3), 389--405.

\bibitem{squirrel2020}
SQUIRREL: Testing Database Management Systems with Language Validity and Coverage Feedback. (2020). arXiv preprint arXiv:2006.02398.

\bibitem{yang2011}
Yang, X., Chen, Y., Eide, E., \& Regehr, J. (2011). Finding and understanding bugs in C compilers. \textit{ACM SIGPLAN Notices}, 46(6), 283--294.

\bibitem{schulz2013}
Schulz, S. (2013). System description: E 1.8. In \textit{LPAR-19: Logic for Programming, Artificial Intelligence, and Reasoning}, pp. 735--743. Springer.

\bibitem{demoura2008}
de Moura, L., \& Bj\o rner, N. (2008). Z3: An efficient SMT solver. In \textit{TACAS 2008: Tools and Algorithms for the Construction and Analysis of Systems}, Lecture Notes in Computer Science, Vol. 4963, pp. 337--340. Springer.

\bibitem{nipkow2002}
Nipkow, T., Paulson, L. C., \& Wenzel, M. (2002). \textit{Isabelle/HOL --- A Proof Assistant for Higher-Order Logic}. Springer.

\bibitem{mirabelle}
Bulwahn, L., \& Krauss, A. (2011). Mirabelle: Automatic testing of automated reasoning tools. In \textit{PAAR 2011: Workshop on Practical Aspects of Automated Reasoning}, pp. 1--5.

\end{thebibliography}

\end{document}

